--- drbd/drbd/drbd_main.c	2023-02-17 14:26:26.762469081 +0000
+++ converted-sources/drbd/drbd_main.c	2023-02-17 14:26:29.154424128 +0000
@@ -10,10 +10,9 @@ 
! header: #define KBUILD_MODNAME ""
    Thanks to Carter Burden, Bart Grantham and Gennadiy Nerubayev
    from Logicworks, Inc. for making SDP replication support possible.
 
-
  */
 
-#define pr_fmt(fmt)	KBUILD_MODNAME ": " fmt
+#define pr_fmt(fmt) ":" fmt
 
 #include <linux/module.h>
 #include <linux/jiffies.h>
@@ -52,13 +51,13 @@
! compat
 #include "drbd_meta_data.h"
 #include "drbd_dax_pmem.h"
 
-static int drbd_open(struct block_device *bdev, fmode_t mode);
+int drbd_open(struct block_device *bdev, fmode_t mode);
 static void drbd_release(struct gendisk *gd, fmode_t mode);
 static void md_sync_timer_fn(struct timer_list *t);
 static int w_bitmap_io(struct drbd_work *w, int unused);
 static int flush_send_buffer(struct drbd_connection *connection, enum drbd_stream drbd_stream);
! header: define __must_hold 
-static u64 __set_bitmap_slots(struct drbd_device *device, u64 bitmap_uuid, u64 do_nodes) __must_hold(local);
-static u64 __test_bitmap_slots(struct drbd_device *device) __must_hold(local);
+static u64 __set_bitmap_slots(struct drbd_device *device, u64 bitmap_uuid, u64 do_nodes) ;
+static u64 __test_bitmap_slots(struct drbd_device *device) ;
 
 MODULE_AUTHOR("Philipp Reisner <phil@linbit.com>, "
 	      "Lars Ellenberg <lars@linbit.com>");
@@ -98,10 +97,11 @@
! compat: /cygdrive/c/windrbd should not be hardcoded .. have ioctl to set this
 unsigned int drbd_minor_count = DRBD_MINOR_COUNT_DEF;
 /* Module parameter for setting the user mode helper program
  * to run. Default is /sbin/drbdadm */
-char drbd_usermode_helper[80] = "/sbin/drbdadm";
+char drbd_usermode_helper[80] = "/cygdrive/c/windrbd/usr/sbin/drbdadm";
 module_param_named(minor_count, drbd_minor_count, uint, 0444);
 module_param_string(usermode_helper, drbd_usermode_helper, sizeof(drbd_usermode_helper), 0644);
 
! compat layer: implement ioctl for setting that
+#if 0
 static int param_set_drbd_protocol_version(const char *s, const struct kernel_param *kp)
 {
 	unsigned long long tmp;
@@ -125,17 +125,18 @@
 	.get = param_get_drbd_protocol_version,
 };
 
+#endif
+
 unsigned int drbd_protocol_version_min = PRO_VERSION_MIN;
 module_param_named(protocol_version_min, drbd_protocol_version_min, drbd_protocol_version, 0644);
 
-
 /* in 2.6.x, our device mapping and config info contains our virtual gendisks
  * as member "struct gendisk *vdisk;"
  */
 struct idr drbd_devices;
 struct list_head drbd_resources;
! header: should already be there ... or compat (need to call a windows function to initialize it)
-DEFINE_SPINLOCK(drbd_devices_lock);
-DEFINE_MUTEX(resources_mutex);
+spinlock_t drbd_devices_lock;
+struct mutex resources_mutex;
 
 struct kmem_cache *drbd_request_cache;
 struct kmem_cache *drbd_ee_cache;	/* peer requests */
@@ -171,17 +172,20 @@
 	return io_allowed;
 }
! compat
+void drbd_cleanup(void);
+
 #endif
 
 struct drbd_connection *__drbd_next_connection_ref(u64 *visited,
 						   struct drbd_connection *connection,
 						   struct drbd_resource *resource)
 {
! cocci: rcu flags script (already exists)
+	KIRQL rcu_flags;
 	int node_id;
 
-	rcu_read_lock();

! cocci: rcu flags script (already exists)
+	rcu_flags = rcu_read_lock();
 	if (!connection) {
! C compiler: block return value gcc, can automate with cocci
-		connection = list_first_or_null_rcu(&resource->connections,
+		list_first_or_null_rcu(connection, &resource->connections,
 						    struct drbd_connection,
 						    connections);
 		*visited = 0;
@@ -221,18 +225,18 @@
 		kref_debug_get(&connection->kref_debug, 13);
 	}
 
! cocci: rcu
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 	return connection;
 }
 
-
 struct drbd_peer_device *__drbd_next_peer_device_ref(u64 *visited,
 						     struct drbd_peer_device *peer_device,
 						     struct drbd_device *device)
 {
! cocci: rcu
-	rcu_read_lock();
+	KIRQL rcu_flags;
+	rcu_flags = rcu_read_lock();
 	if (!peer_device) {
! cocci: block return value
-		peer_device = list_first_or_null_rcu(&device->peer_devices,
+		list_first_or_null_rcu(peer_device, &device->peer_devices,
 						    struct drbd_peer_device,
 						    peer_devices);
 		*visited = 0;
@@ -268,7 +272,7 @@
 		kref_debug_get(&peer_device->connection->kref_debug, 15);
 	}
 
! cocci: rcu
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 	return peer_device;
 }
 
@@ -296,7 +300,7 @@ cocci: typeof
 					       struct list_head *transfer_log)
 {
 	if (req) {
-		struct drbd_request *next = list_next_entry(req, tl_requests);
+		struct drbd_request *next = list_next_entry(struct drbd_request, req, tl_requests);
 		if (&next->tl_requests != transfer_log)
 			kref_get(&next->kref);
 		*pnext = next;
@@ -315,7 +319,7 @@ cocci: typeof
 		if (next_is_head)
 			return NULL;
 		req = next;
-		next = list_next_entry(req, tl_requests);
+		next = list_next_entry(struct drbd_request, req, tl_requests);
 		next_is_head = (&next->tl_requests == transfer_log);
 		if (!next_is_head)
 			kref_get(&next->kref);
@@ -361,6 +365,7 @@
 		unsigned int barrier_nr,
 		unsigned int set_size)
 {
! cocci: spin lock flags
+	KIRQL spin_lock_irq_flags;
 	struct drbd_resource *resource = connection->resource;
 	struct drbd_request *r;
 	struct drbd_request *req = NULL;
@@ -369,11 +374,11 @@
 	int expect_size = 0;
 	const int idx = connection->peer_node_id;
 
! cocci: spin lock flags
-	spin_lock_irq(&resource->req_lock);
+	spin_lock_irqsave(&resource->req_lock, spin_lock_irq_flags);
 
 	/* find oldest not yet barrier-acked write request,
 	 * count writes in its epoch. */
! cocci: typeof
-	list_for_each_entry(r, &resource->transfer_log, tl_requests) {
+	list_for_each_entry(struct drbd_request, r, &resource->transfer_log, tl_requests) {
 		if (!req) {
 			if (!(r->local_rq_state & RQ_WRITE))
 				continue;
! cocci: long -> ULONG_PTR
@@ -404,7 +409,7 @@
 			}
 			expect_size++;
 		}
-		if (y_block_id && (struct drbd_request*)(unsigned long)y_block_id == r) {
+		if (y_block_id && (struct drbd_request*)(ULONG_PTR)y_block_id == r) {
 			req_y = r;
 			break;
 		}
! cocci: long -> ULONG_PTR
@@ -412,14 +417,14 @@
 
 	/* first some paranoia code */
 	if (o_block_id) {
-		if ((struct drbd_request*)(unsigned long)o_block_id != req) {
+		if ((struct drbd_request*)(ULONG_PTR)o_block_id != req) {
 			drbd_err(connection, "BAD! ConfirmedStable: expected %p, found %p\n",
-				(struct drbd_request*)(unsigned long)o_block_id, req);
! cocci: long -> ULONG_PTR
+				(struct drbd_request*)(ULONG_PTR)o_block_id, req);
 			goto bail;
 		}
 		if (!req_y) {
 			drbd_err(connection, "BAD! ConfirmedStable: expected youngest request %p NOT found\n",
-				(struct drbd_req*)(unsigned long)y_block_id);
! cocci: long -> ULONG_PTR
+				(struct drbd_req*)(ULONG_PTR)y_block_id);
 			goto bail;
 		}
 		/* A P_CONFIRM_STABLE cannot tell me the to-be-expected barrier nr,
@@ -450,7 +455,7 @@
 				 barrier_nr, set_size, expect_size);
 #if 0
 /* DEBUGGING AID */
-			list_for_each_entry(req, &resource->transfer_log, tl_requests)
! cocci: typeof
+			list_for_each_entry(struct drbd_request, req, &resource->transfer_log, tl_requests)
 				if (req->epoch == expect_epoch)
 					break;
 			list_for_each_entry_from(req, &resource->transfer_log, tl_requests) {
@@ -473,7 +478,7 @@
 	/* this extra list walk restart is paranoia,
 	 * to catch requests being barrier-acked "unexpectedly".
 	 * It usually should find the same req again, or some READ preceding it. */
! cocci: typeof
-	list_for_each_entry(req, &resource->transfer_log, tl_requests)
+	list_for_each_entry(struct drbd_request, req, &resource->transfer_log, tl_requests)
 		if (req->epoch == expect_epoch)
 			break;
 	tl_for_each_req_ref_from(req, r, &resource->transfer_log) {
@@ -489,7 +494,7 @@
 			break;
 		}
 	}
! cocci: spinlock
-	spin_unlock_irq(&resource->req_lock);
+	spin_unlock_irqrestore(&resource->req_lock, spin_lock_irq_flags);
 
 	/* urgently flush out peer acks for P_CONFIRM_STABLE */
 	if (req_y) {
@@ -502,11 +507,10 @@
 	return;
 
 bail:
! cocci: spinlock
-	spin_unlock_irq(&resource->req_lock);
+	spin_unlock_irqrestore(&resource->req_lock, spin_lock_irq_flags);
 	change_cstate(connection, C_PROTOCOL_ERROR, CS_HARD);
 }
 
-
 /**
  * _tl_walk() - Walks the transfer log, and applies an action to all requests
  * @connection:	DRBD connection to operate on.
@@ -537,11 +541,12 @@
 
 void tl_walk(struct drbd_connection *connection, enum drbd_req_event what)
 {
! cocci: spinlock
+	KIRQL spin_lock_irq_flags;
 	struct drbd_resource *resource = connection->resource;
 
! cocci: spinlock
-	spin_lock_irq(&resource->req_lock);
+	spin_lock_irqsave(&resource->req_lock, spin_lock_irq_flags);
 	_tl_walk(connection, what);
! cocci: spinlock
-	spin_unlock_irq(&resource->req_lock);
+	spin_unlock_irqrestore(&resource->req_lock, spin_lock_irq_flags);
 }
 
 /**
@@ -550,10 +555,11 @@
  */
 void tl_abort_disk_io(struct drbd_device *device)
 {
! cocci: spinlock
+        KIRQL spin_lock_irq_flags;
         struct drbd_resource *resource = device->resource;
         struct drbd_request *req, *r;
 
! cocci: spinlock
-        spin_lock_irq(&resource->req_lock);
+        spin_lock_irqsave(&resource->req_lock, spin_lock_irq_flags);
         tl_for_each_req_ref(req, r, &resource->transfer_log) {
                 if (!(req->local_rq_state & RQ_LOCAL_PENDING))
                         continue;
@@ -561,15 +567,15 @@
                         continue;
                 _req_mod(req, ABORT_DISK_IO, NULL);
         }
! cocci: spinlock
-        spin_unlock_irq(&resource->req_lock);
+        spin_unlock_irqrestore(&resource->req_lock, spin_lock_irq_flags);
 }
 
-static int drbd_thread_setup(void *arg)
+int drbd_thread_setup(void *arg)
 {
 	struct drbd_thread *thi = (struct drbd_thread *) arg;
 	struct drbd_resource *resource = thi->resource;
 	struct drbd_connection *connection = thi->connection;
! cocci: spinlock
-	unsigned long flags;
+	KIRQL flags;
 	int retval;
 
 	allow_kernel_signal(DRBD_SIGKILL);
@@ -614,6 +620,7 @@
 	else
 		drbd_info(resource, "Terminating %s thread\n", thi->name);
 
! remove
+		/* TODO: this is racy. */
 	complete(&thi->stop);
 	spin_unlock_irqrestore(&thi->t_lock, flags);
 
@@ -642,7 +649,7 @@
 	struct drbd_resource *resource = thi->resource;
 	struct drbd_connection *connection = thi->connection;
 	struct task_struct *nt;
! cocci: spinlock
-	unsigned long flags;
+	KIRQL flags;
 
 	/* is used from state engine doing drbd_thread_stop_nowait,
 	 * while holding the req lock irqsave */
@@ -700,10 +707,9 @@
 	return true;
 }
 
-
 void _drbd_thread_stop(struct drbd_thread *thi, int restart, int wait)
 {
! cocci: spinlock
-	unsigned long flags;
+	KIRQL flags;
 
 	enum drbd_thread_state ns = restart ? RESTARTING : EXITING;
 
@@ -750,19 +756,20 @@
  */
 static void drbd_calc_cpu_mask(cpumask_var_t *cpu_mask)
 {
! cocci: spinlock
+	KIRQL rcu_flags;
 	unsigned int *resources_per_cpu, min_index = ~0;
 
! remove: no Tag in kmalloc
-	resources_per_cpu = kzalloc(nr_cpu_ids * sizeof(*resources_per_cpu), GFP_KERNEL);
+	resources_per_cpu = kzalloc(nr_cpu_ids * sizeof(*resources_per_cpu), GFP_KERNEL, '00WD');
 	if (resources_per_cpu) {
 		struct drbd_resource *resource;
 		unsigned int cpu, min = ~0;
 
! cocci: RCU lock
-		rcu_read_lock();
+		rcu_flags = rcu_read_lock();
 		for_each_resource_rcu(resource, &drbd_resources) {
 			for_each_cpu(cpu, resource->cpu_mask)
 				resources_per_cpu[cpu]++;
 		}
! cocci: RCU lock
-		rcu_read_unlock();
+		rcu_read_unlock(rcu_flags);
 		for_each_online_cpu(cpu) {
 			if (resources_per_cpu[cpu] < min) {
 				min = resources_per_cpu[cpu];
@@ -796,17 +803,18 @@
 	set_cpus_allowed_ptr(p, resource->cpu_mask);
 }
 #else
! remove: GNU extension, always CONFIG_SMP anyway
-#define drbd_calc_cpu_mask(A) ({})
+#define drbd_calc_cpu_mask(A) {}
 #endif
 
 static bool drbd_all_neighbor_secondary(struct drbd_device *device, u64 *authoritative_ptr)
 {
! cocci: RCU lock
+	KIRQL rcu_flags;
 	struct drbd_peer_device *peer_device;
 	bool all_secondary = true;
 	u64 authoritative = 0;
 	int id;
 
! cocci: RCU lock
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_peer_device_rcu(peer_device, device) {
 		if (peer_device->repl_state[NOW] >= L_ESTABLISHED &&
 		    peer_device->connection->peer_role[NOW] == R_PRIMARY) {
@@ -815,7 +823,7 @@
 			authoritative |= NODE_MASK(id);
 		}
 	}
! cocci: RCU lock
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 	if (authoritative_ptr)
 		*authoritative_ptr = authoritative;
 	return all_secondary;
@@ -895,17 +903,19 @@
 		prepare_header80(buffer, cmd, size);
 }
 
! manual Oh wow this is part of the variable page size patch ... hmmm. 
-static void new_or_recycle_send_buffer_page(struct drbd_send_buffer *sbuf)
+static void new_or_recycle_send_buffer_page(struct drbd_send_buffer *sbuf, size_t size)
 {
 	while (1) {
 		struct page *page;
 		int count = page_count(sbuf->page);
 
! manual Oh wow this is part of the variable page size patch ... hmmm. 
-		BUG_ON(count == 0);
-		if (count == 1)
-			goto have_page;
+		if (sbuf->page->size >= size) {
+			BUG_ON(count == 0);
+			if (count == 1)
+				goto have_page;
+		}
 
! manual Oh wow this is part of the variable page size patch ... hmmm. 
-		page = alloc_page(GFP_NOIO | __GFP_NORETRY | __GFP_NOWARN);
+		page = alloc_page_of_size(GFP_NOIO | __GFP_NORETRY | __GFP_NOWARN, size);
 		if (page) {
 			put_page(sbuf->page);
 			sbuf->page = page;
@@ -925,9 +935,9 @@
 	struct drbd_send_buffer *sbuf = &connection->send_buffer[drbd_stream];
 	char *page_start = page_address(sbuf->page);
 
! manual Oh wow this is part of the variable page size patch ... hmmm. 
-	if (sbuf->pos - page_start + size > PAGE_SIZE) {
+	if (sbuf->pos - page_start + size > sbuf->page->size) {
 		flush_send_buffer(connection, drbd_stream);
! manual Oh wow this is part of the variable page size patch ... hmmm. 
-		new_or_recycle_send_buffer_page(sbuf);
+		new_or_recycle_send_buffer_page(sbuf, size);
 	}
 
 	sbuf->allocated_size = size;
@@ -1010,6 +1020,7 @@
 
 static int flush_send_buffer(struct drbd_connection *connection, enum drbd_stream drbd_stream)
 {
! cocci: RCU lock
+	KIRQL rcu_flags;
 	struct drbd_send_buffer *sbuf = &connection->send_buffer[drbd_stream];
 	struct drbd_transport *transport = &connection->transport;
 	struct drbd_transport_ops *tr_ops = transport->ops;
@@ -1020,9 +1031,9 @@
 		return 0;
 
 	if (drbd_stream == DATA_STREAM) {
! cocci: RCU lock
-		rcu_read_lock();
+		rcu_flags = rcu_read_lock();
 		connection->transport.ko_count = rcu_dereference(connection->transport.net_conf)->ko_count;
! cocci: RCU lock
-		rcu_read_unlock();
+		rcu_read_unlock(rcu_flags);
 	}
 
 	flags = (connection->cstate[NOW] < C_CONNECTING ? MSG_DONTWAIT : 0) |
@@ -1096,7 +1107,6 @@
 	struct drbd_transport *transport = &connection->transport;
 	struct drbd_transport_ops *tr_ops = transport->ops;
 
! remove
-
 	mutex_lock(&connection->mutex[stream]);
 	flush_send_buffer(connection, stream);
 
@@ -1141,6 +1151,7 @@
 int drbd_send_peer_ack(struct drbd_connection *connection,
 			      struct drbd_request *req)
 {
! cocci: RCU lock
+	KIRQL rcu_flags;
 	struct drbd_resource *resource = connection->resource;
 	struct drbd_connection *c;
 	struct p_peer_ack *p;
@@ -1149,14 +1160,14 @@
 	if (req->local_rq_state & RQ_LOCAL_OK)
 		mask |= NODE_MASK(resource->res_opts.node_id);
 
! cocci: RCU lock
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_connection_rcu(c, resource) {
 		int node_id = c->peer_node_id;
 
 		if (req->net_rq_state[node_id] & RQ_NET_OK)
 			mask |= NODE_MASK(node_id);
 	}
! cocci: RCU lock
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	p = conn_prepare_command(connection, sizeof(*p), CONTROL_STREAM);
 	if (!p)
@@ -1169,6 +1180,7 @@
 
 int drbd_send_sync_param(struct drbd_peer_device *peer_device)
 {
! cocci: RCU lock
+	KIRQL rcu_flags;
 	struct p_rs_param_95 *p;
 	int size;
 	const int apv = peer_device->connection->agreed_pro_version;
@@ -1176,7 +1188,7 @@
 	struct net_conf *nc;
 	struct peer_device_conf *pdc;
 
! cocci: RCU lock
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	nc = rcu_dereference(peer_device->connection->transport.net_conf);
 
 	size = apv <= 87 ? sizeof(struct p_rs_param)
@@ -1186,7 +1198,7 @@
 		: /* apv >= 95 */ sizeof(struct p_rs_param_95);
 
 	cmd = apv >= 89 ? P_SYNC_PARAM89 : P_SYNC_PARAM;
! cocci: RCU lock
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	p = drbd_prepare_command(peer_device, size, DATA_STREAM);
 	if (!p)
@@ -1195,7 +1207,7 @@
 	/* initialize verify_alg and csums_alg */
 	memset(p->verify_alg, 0, 2 * SHARED_SECRET_MAX);
 
! cocci: RCU lock
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	nc = rcu_dereference(peer_device->connection->transport.net_conf);
 
 	if (get_ldev(peer_device->device)) {
@@ -1218,13 +1230,14 @@
 		strcpy(p->verify_alg, nc->verify_alg);
 	if (apv >= 89)
 		strcpy(p->csums_alg, nc->csums_alg);
! cocci: RCU lock
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return drbd_send_command(peer_device, cmd, DATA_STREAM);
 }
 
 int __drbd_send_protocol(struct drbd_connection *connection, enum drbd_packet cmd)
 {
! cocci: RCU lock
+	KIRQL rcu_flags;
 	struct p_protocol *p;
 	struct net_conf *nc;
 	int size, cf;
@@ -1236,17 +1249,17 @@
 	}
 
 	size = sizeof(*p);
! cocci: RCU lock
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	nc = rcu_dereference(connection->transport.net_conf);
 	if (connection->agreed_pro_version >= 87)
 		size += strlen(nc->integrity_alg) + 1;
! cocci: RCU lock
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	p = __conn_prepare_command(connection, size, DATA_STREAM);
 	if (!p)
 		return -EIO;
 
! cocci: RCU lock
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	nc = rcu_dereference(connection->transport.net_conf);
 
 	p->protocol      = cpu_to_be32(nc->wire_protocol);
@@ -1263,7 +1276,7 @@
 
 	if (connection->agreed_pro_version >= 87)
 		strcpy(p->integrity_alg, nc->integrity_alg);
! cocci: RCU lock
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return __send_command(connection, -1, cmd, DATA_STREAM);
 }
@@ -1281,6 +1294,7 @@
 
 static int _drbd_send_uuids(struct drbd_peer_device *peer_device, u64 uuid_flags)
 {
! cocci: spin lock
+	KIRQL spin_lock_irq_flags;
 	struct drbd_device *device = peer_device->device;
 	struct p_uuids *p;
 	int i;
@@ -1294,13 +1308,14 @@
 		return -EIO;
 	}
 
! cocci: spin lock
-	spin_lock_irq(&device->ldev->md.uuid_lock);
+	spin_lock_irqsave(&device->ldev->md.uuid_lock, spin_lock_irq_flags);
 	peer_device->comm_current_uuid = drbd_current_uuid(device);
 	p->current_uuid = cpu_to_be64(peer_device->comm_current_uuid);
 	p->bitmap_uuid = cpu_to_be64(drbd_bitmap_uuid(peer_device));
 	for (i = 0; i < ARRAY_SIZE(p->history_uuids); i++)
 		p->history_uuids[i] = cpu_to_be64(drbd_history_uuid(device, i));
! cocci: spin lock
-	spin_unlock_irq(&device->ldev->md.uuid_lock);
+	spin_unlock_irqrestore(&device->ldev->md.uuid_lock,
+			       spin_lock_irq_flags);
 
 	peer_device->comm_bm_set = drbd_bm_total_weight(peer_device);
 	p->dirty_bits = cpu_to_be64(peer_device->comm_bm_set);
@@ -1325,8 +1340,9 @@
 	return drbd_send_command(peer_device, P_UUIDS, DATA_STREAM);
 }
 
! header: define __must_hold
-static u64 __bitmap_uuid(struct drbd_device *device, int node_id) __must_hold(local)
+static u64 __bitmap_uuid(struct drbd_device *device, int node_id) 
 {
! cocci: rcu lock
+	KIRQL rcu_flags;
 	struct drbd_peer_device *peer_device;
 	struct drbd_peer_md *peer_md = device->ldev->md.peers;
 	u64 bitmap_uuid = peer_md[node_id].bitmap_uuid;
@@ -1345,7 +1361,7 @@
 	   Exceptions to the above are when the peer's UUID is not known yet
 	 */
 
! cocci: rcu lock
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	peer_device = peer_device_by_node_id(device, node_id);
 	if (peer_device) {
 		enum drbd_repl_state repl_state = peer_device->repl_state[NOW];
@@ -1356,7 +1372,7 @@
 		    (drbd_current_uuid(device) & ~UUID_PRIMARY))
 			bitmap_uuid = -1;
 	}
! cocci: rcu lock
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return bitmap_uuid;
 }
@@ -1385,11 +1401,12 @@
 /* sets UUID_FLAG_SYNC_TARGET on uuid_flags as appropriate (may be NULL) */
 u64 drbd_resolved_uuid(struct drbd_peer_device *peer_device_base, u64 *uuid_flags)
 {
! cocci: rcu lock
+	KIRQL rcu_flags;
 	struct drbd_device *device = peer_device_base->device;
 	struct drbd_peer_device *peer_device;
 	u64 uuid = drbd_current_uuid(device);
 
-	rcu_read_lock();
! cocci: rcu lock
+	rcu_flags = rcu_read_lock();
 	for_each_peer_device_rcu(peer_device, device) {
 		if (peer_device->node_id == peer_device_base->node_id)
 			continue;
@@ -1400,13 +1417,14 @@
 			break;
 		}
 	}
-	rcu_read_unlock();
! cocci: rcu lock
+	rcu_read_unlock(rcu_flags);
 
 	return uuid;
 }
 
 static int _drbd_send_uuids110(struct drbd_peer_device *peer_device, u64 uuid_flags, u64 node_mask)
 {
! cocci: spin lock
+	KIRQL spin_lock_irq_flags;
 	struct drbd_device *device = peer_device->device;
 	const int my_node_id = device->resource->res_opts.node_id;
 	struct drbd_peer_md *peer_md;
@@ -1429,7 +1447,7 @@
 		return -EIO;
 	}
 
! cocci: spin lock
-	spin_lock_irq(&device->ldev->md.uuid_lock);
+	spin_lock_irqsave(&device->ldev->md.uuid_lock, spin_lock_irq_flags);
 	peer_device->comm_current_uuid = drbd_resolved_uuid(peer_device, &local_uuid_flags);
 	p->current_uuid = cpu_to_be64(peer_device->comm_current_uuid);
 
@@ -1455,7 +1473,8 @@
 
 	for (i = 0; i < HISTORY_UUIDS; i++)
 		p->other_uuids[pos++] = cpu_to_be64(drbd_history_uuid(device, i));
! cocci: spin lock
-	spin_unlock_irq(&device->ldev->md.uuid_lock);
+	spin_unlock_irqrestore(&device->ldev->md.uuid_lock,
+			       spin_lock_irq_flags);
 
 	p->bitmap_uuids_mask = cpu_to_be64(bitmap_uuids_mask);
 
@@ -1549,7 +1568,7 @@
 }
 
 /* All callers hold resource->conf_update */
! header: define __must_hold
-int drbd_attach_peer_device(struct drbd_peer_device *const peer_device) __must_hold(local)
+int drbd_attach_peer_device(struct drbd_peer_device *peer_device)
 {
 	struct lru_cache *resync_lru = NULL;
 	int err = -ENOMEM;
@@ -1593,6 +1612,7 @@
 int drbd_send_sizes(struct drbd_peer_device *peer_device,
 		    uint64_t u_size_diskless, enum dds_flags flags)
 {
! cocci: rcu lock
+	KIRQL rcu_flags;
 	struct drbd_device *device = peer_device->device;
 	struct p_sizes *p;
 	sector_t d_size, u_size;
@@ -1612,9 +1632,9 @@
 	if (get_ldev_if_state(device, D_NEGOTIATING)) {
 		struct request_queue *q = bdev_get_queue(device->ldev->backing_bdev);
 		d_size = drbd_get_max_capacity(device, device->ldev, false);
! cocci: rcu lock
-		rcu_read_lock();
+		rcu_flags = rcu_read_lock();
 		u_size = rcu_dereference(device->ldev->disk_conf)->disk_size;
! cocci: rcu lock
-		rcu_read_unlock();
+		rcu_read_unlock(rcu_flags);
 		q_order_type = drbd_queue_order_type(device);
 		max_bio_size = queue_max_hw_sectors(q) << 9;
 		max_bio_size = min(max_bio_size, DRBD_MAX_BIO_SIZE);
@@ -1815,18 +1835,19 @@
 				unsigned int size,
 				struct bm_xfer_ctx *c)
 {
! cocci: rcu lock
+	KIRQL rcu_flags;
 	struct bitstream bs;
-	unsigned long plain_bits;
-	unsigned long tmp;
-	unsigned long rl;
! cocci: ULONG_PTR
+	ULONG_PTR plain_bits;
+	ULONG_PTR tmp;
+	ULONG_PTR rl;
 	unsigned len;
 	unsigned toggle;
 	int bits, use_rle;
 
 	/* may we use this feature? */
! cocci: rcu lock
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	use_rle = rcu_dereference(peer_device->connection->transport.net_conf)->use_rle;
! cocci: rcu lock
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 	if (!use_rle || peer_device->connection->agreed_pro_version < 90)
 		return 0;
 
@@ -1849,7 +1870,7 @@
 	do {
 		tmp = (toggle == 0) ? _drbd_bm_find_next_zero(peer_device, c->bit_offset)
 				    : _drbd_bm_find_next(peer_device, c->bit_offset);
! cocci: ulong const
-		if (tmp == -1UL)
+		if (tmp == ((ULONG_PTR)-1))
 			tmp = c->bm_bits;
 		rl = tmp - c->bit_offset;
 
@@ -1943,8 +1964,8 @@
 		/* was not compressible.
 		 * send a buffer full of plain text bits instead. */
 		unsigned int data_size;
! cocci: ulong
-		unsigned long num_words;
-		unsigned long *pu = (unsigned long *)pc;
+		ULONG_PTR num_words;
+		ULONG_PTR *pu = (ULONG_PTR *)pc;
 
 		data_size = DRBD_SOCKET_BUFFER_SIZE - header_size;
 		num_words = min_t(size_t, data_size / sizeof(*pu),
@@ -1988,7 +2009,7 @@
 	if (get_ldev(device)) {
 		if (drbd_md_test_peer_flag(peer_device, MDF_PEER_FULL_SYNC)) {
 			drbd_info(device, "Writing the whole bitmap, MDF_FullSync was set.\n");
-			drbd_bm_set_many_bits(peer_device, 0, -1UL);
! cocci: ulong const
+			drbd_bm_set_many_bits(peer_device, 0, ((ULONG_PTR)-1));
 			if (drbd_bm_write(device, NULL)) {
 				/* write_bm did fail! Leave full sync flag set in Meta P_DATA
 				 * but otherwise process as per normal - need to tell other
@@ -2155,19 +2176,20 @@
 static int _drbd_send_bio(struct drbd_peer_device *peer_device, struct bio *bio)
 {
 	struct drbd_connection *connection = peer_device->connection;
! compat: this probably should be changed in compat layer
-	struct bio_vec bvec;
-	struct bvec_iter iter;
+	struct bio_vec *bvec;
+	int iter;
 
 	/* Flush send buffer and make sure PAGE_SIZE is available... */
 	alloc_send_buffer(connection, PAGE_SIZE, DATA_STREAM);
 	connection->send_buffer[DATA_STREAM].allocated_size = 0;
 
 	/* hint all but last page with MSG_MORE */
! compat: this probably should be changed in compat layer
+	iter = 0;
 	bio_for_each_segment(bvec, bio, iter) {
 		int err;
 
! compat: this probably should be changed in compat layer
-		err = _drbd_no_send_page(peer_device, bvec.bv_page,
-					 bvec.bv_offset, bvec.bv_len,
+		err = _drbd_no_send_page(peer_device, bvec->bv_page,
+					 bvec->bv_offset, bvec->bv_len,
 					 bio_iter_last(bvec, iter) ? 0 : MSG_MORE);
 		if (err)
 			return err;
@@ -2175,15 +2197,15 @@
 		if (bio_op(bio) == REQ_OP_WRITE_SAME)
 			break;
 
! compat: this probably should be changed in compat layer
-		peer_device->send_cnt += bvec.bv_len >> 9;
+		peer_device->send_cnt += bvec->bv_len >> 9;
 	}
 	return 0;
 }
 
 static int _drbd_send_zc_bio(struct drbd_peer_device *peer_device, struct bio *bio)
 {
-	struct bio_vec bvec;
-	struct bvec_iter iter;
! compat: this probably should be changed in compat layer
+	struct bio_vec *bvec;
+	int iter;
 	bool no_zc = drbd_disable_sendpage;
 
 	/* e.g. XFS meta- & log-data is in slab pages, which have a
@@ -2194,7 +2216,7 @@
 	 * by someone, leading to some obscure delayed Oops somewhere else. */
 	if (!no_zc)
 		bio_for_each_segment(bvec, bio, iter) {
! compat: this probably should be changed in compat layer
-			struct page *page = bvec.bv_page;
+			struct page *page = bvec->bv_page;
 
 			if (!sendpage_ok(page)) {
 				no_zc = true;
@@ -2309,7 +2331,7 @@
 	}
 
 	p->sector = cpu_to_be64(req->i.sector);
! cocci: unsigned long
-	p->block_id = (unsigned long)req;
+	p->block_id = (ULONG_PTR)req;
 	p->seq_num = cpu_to_be32(atomic_inc_return(&peer_device->packet_seq));
 	dp_flags = bio_flags_to_wire(peer_device->connection, req->master_bio);
 	if (peer_device->repl_state[NOW] >= L_SYNC_SOURCE && peer_device->repl_state[NOW] <= L_PAUSED_SYNC_T)
@@ -2334,10 +2356,13 @@
 		memcpy(digest_out, before, digest_size);
 	}
 
! compat: implement write same
+	err = 0;
 	if (wsame) {
! compat: implement write same
+#if 0
 		additional_size_command(peer_device->connection, DATA_STREAM,
 					bio_iovec(req->master_bio).bv_len);
 		err = __send_command(peer_device->connection, device->vnr, P_WSAME, DATA_STREAM);
! compat: implement write same
+#endif
 	} else {
 		additional_size_command(peer_device->connection, DATA_STREAM, req->i.size);
 		err = __send_command(peer_device->connection, device->vnr, P_DATA, DATA_STREAM);
@@ -2354,10 +2379,13 @@
 		 * out ok after sending on this side, but does not fit on the
 		 * receiving side, we sure have detected corruption elsewhere.
 		 */
! compat: implement zero copy
-		if (!(s & (RQ_EXP_RECEIVE_ACK | RQ_EXP_WRITE_ACK)) || digest_size)
-			err = _drbd_send_bio(peer_device, req->master_bio);
-		else
-			err = _drbd_send_zc_bio(peer_device, req->master_bio);
+
+		/* For WinDRBD this currently always uses the copy variant,
+		 * since zero copy is currently broken and BSOD's from
+		 * time to time.
+		 */
+
+		err = _drbd_send_bio(peer_device, req->master_bio);
 
 		/* double check digest, sometimes buffers have been modified in flight. */
 		if (digest_size > 0) {
@@ -2438,11 +2466,12 @@
! cocci: rcu flags
 /* primary_peer_present_and_not_two_primaries_allowed() */
 static bool primary_peer_present(struct drbd_resource *resource)
 {
+	KIRQL rcu_flags;
 	struct drbd_connection *connection;
 	struct net_conf *nc;
 	bool two_primaries, rv = false;
 
-	rcu_read_lock();
! cocci: rcu flags
+	rcu_flags = rcu_read_lock();
 	for_each_connection_rcu(connection, resource) {
 		nc = rcu_dereference(connection->transport.net_conf);
 		two_primaries = nc ? nc->two_primaries : false;
@@ -2452,16 +2481,17 @@
 			break;
 		}
 	}
! cocci: rcu flags
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return rv;
 }
 
 static bool any_disk_is_uptodate(struct drbd_device *device)
 {
! cocci: rcu flags
+	KIRQL rcu_flags;
 	bool ret = false;
 
! cocci: rcu flags
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	if (device->disk_state[NOW] == D_UP_TO_DATE)
 		ret = true;
 	else {
@@ -2474,13 +2504,13 @@
 			}
 		}
 	}
! cocci: rcu flags
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return ret;
 }
 
 /* If we are trying to (re-)establish some connection,
! remove
- * it may be useful to re-try the conditions in drbd_open().
+ * it may be useful to re-try_ the conditions in drbd_open().
  * But if we have no connection at all (yet/anymore),
  * or are disconnected and not trying to (re-)establish,
  * or are established already, retrying won't help at all.
@@ -2492,9 +2522,10 @@
  */
 static bool connection_state_may_improve_soon(struct drbd_resource *resource)
 {
! cocci: rcu flags
+	KIRQL rcu_flags;
 	struct drbd_connection *connection;
 	bool ret = false;
! cocci: rcu flags
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_connection_rcu(connection, resource) {
 		enum drbd_conn_state cstate = connection->cstate[NOW];
 		if (C_DISCONNECTING < cstate && cstate < C_CONNECTED) {
@@ -2502,14 +2533,14 @@
 			break;
 		}
 	}
! cocci: rcu flags
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 	return ret;
 }
 
! compat: call via drbd_open(bdev, FMODE_WRITE)?
-static int try_to_promote(struct drbd_device *device, long timeout, bool ndelay)
+int try_to_promote(struct drbd_device *device, LONG_PTR timeout, bool ndelay)
 {
 	struct drbd_resource *resource = device->resource;
! remove
-	int rv, retry = timeout / (HZ / 5); /* One try every 200ms */
+	int rv, retry = timeout / (HZ / 5); /* One try_ every 200ms */
 	do {
 		rv = drbd_set_role(resource, R_PRIMARY, false, NULL);
 		if (ndelay)
@@ -2518,22 +2549,24 @@
 			return rv;
 		} else if (rv == SS_CW_FAILED_BY_PEER) {
 			/* Probably udev has it open read-only on one of the peers */
! cocci: unsigned long
-			long t = schedule_timeout_interruptible(HZ / 5);
+			LONG_PTR t = schedule_timeout_interruptible(HZ / 5);
 			if (t < 0)
 				break;
 			timeout -= HZ / 5;
 		} else if (rv == SS_TWO_PRIMARIES) {
 			/* Wait till the peer demoted itself */
! cocci: no returnvalue from block
-			timeout = wait_event_interruptible_timeout(resource->state_wait,
-				resource->role[NOW] == R_PRIMARY ||
-				(!primary_peer_present(resource) && any_disk_is_uptodate(device)),
-				timeout);
+			wait_event_interruptible_timeout(timeout,
+							 resource->state_wait,
+							 resource->role[NOW] == R_PRIMARY || (!primary_peer_present(resource) && any_disk_is_uptodate(device)),
+							 timeout);
 			if (timeout <= 0)
 				break;
 		} else if (rv == SS_NO_UP_TO_DATE_DISK && connection_state_may_improve_soon(resource)) {
 			/* Wait until we get a connection established */
! cocci: no returnvalue from block
-			timeout = wait_event_interruptible_timeout(resource->state_wait,
-				any_disk_is_uptodate(device), timeout);
+			wait_event_interruptible_timeout(timeout,
+							 resource->state_wait,
+							 any_disk_is_uptodate(device),
+							 timeout);
 			if (timeout <= 0)
 				break;
 		} else {
@@ -2568,13 +2601,14 @@
 
 static enum ioc_rv inc_open_count(struct drbd_device *device, fmode_t mode)
 {
! cocci: spin lock
+	KIRQL spin_lock_irq_flags;
 	struct drbd_resource *resource = device->resource;
 	enum ioc_rv r = mode & FMODE_NDELAY ? IOC_ABORT : IOC_SLEEP;
 
 	if (test_bit(DOWN_IN_PROGRESS, &resource->flags))
 		return IOC_ABORT;
 
! cocci: spin lock
-	spin_lock_irq(&resource->req_lock);
+	spin_lock_irqsave(&resource->req_lock, spin_lock_irq_flags);
 	if (test_bit(UNREGISTERED, &device->flags))
 		r = IOC_ABORT;
 	else if (!resource->remote_state_change) {
@@ -2584,7 +2618,7 @@
 		else
 			device->open_ro_cnt++;
 	}
! cocci: spin lock
-	spin_unlock_irq(&resource->req_lock);
+	spin_unlock_irqrestore(&resource->req_lock, spin_lock_irq_flags);
 
 	return r;
 }
@@ -2593,7 +2627,7 @@
 {
 	struct opener *pos, *tmp;
 
! cocci: typeof
-	list_for_each_entry_safe(pos, tmp, &device->openers, list) {
+	list_for_each_entry_safe(struct opener, pos, tmp, &device->openers, list) {
 		// if pid == 0, i.e., counts were 0, delete all entries, else the matching one
 		if (pid == 0 || pid == pos->pid) {
 			dynamic_drbd_dbg(device, "%sopeners del: %s(%d)\n", pid == 0 ? "" : "all ",
@@ -2616,25 +2650,27 @@
 
 static void prune_or_free_openers(struct drbd_device *device, pid_t pid)
 {
! cocci: spin lock
-	spin_lock(&device->openers_lock);
+	KIRQL spin_lock_flags;
+	spin_lock_irqsave(&device->openers_lock, spin_lock_flags);
 	__prune_or_free_openers(device, pid);
! cocci: spin lock
-	spin_unlock(&device->openers_lock);
+	spin_unlock_irqrestore(&device->openers_lock, spin_lock_flags);
 }
 
 static void add_opener(struct drbd_device *device)
 {
! cocci: spin lock
+	KIRQL spin_lock_flags;
 	struct opener *opener, *tmp;
 	int len = 0;
 
! remove: no kmalloc tag
-	opener = kmalloc(sizeof(*opener), GFP_NOIO);
+	opener = kmalloc(sizeof(*opener), GFP_NOIO, '01WD');
 	if (!opener)
 		return;
 	get_task_comm(opener->comm, current);
 	opener->pid = task_pid_nr(current);
 	opener->opened = ktime_get_real();
 
! cocci: spin lock
-	spin_lock(&device->openers_lock);
-	list_for_each_entry(tmp, &device->openers, list)
+	spin_lock_irqsave(&device->openers_lock, spin_lock_flags);
! cocci: typeof
+	list_for_each_entry(struct opener, tmp, &device->openers, list)
 		if (++len > 100) { /* 100 ought to be enough for everybody */
 			dynamic_drbd_dbg(device, "openers: list full, do not add new opener\n");
 			kfree(opener);
@@ -2644,14 +2680,15 @@
 	list_add(&opener->list, &device->openers);
 	dynamic_drbd_dbg(device, "openers add: %s(%d)\n", opener->comm, opener->pid);
 out:
! cocci: spin lock
-	spin_unlock(&device->openers_lock);
+	spin_unlock_irqrestore(&device->openers_lock, spin_lock_flags);
 }
 
! compat: call via drbd_ops
-static int drbd_open(struct block_device *bdev, fmode_t mode)
+int drbd_open(struct block_device *bdev, fmode_t mode)
 {
+	long remaining_time;
 	struct drbd_device *device = bdev->bd_disk->private_data;
 	struct drbd_resource *resource = device->resource;
! cocci
-	long timeout = resource->res_opts.auto_promote_timeout * HZ / 10;
+	LONG_PTR timeout = resource->res_opts.auto_promote_timeout * HZ / 10;
 	enum ioc_rv r;
 	int err = 0;
 
@@ -2673,9 +2710,9 @@
 
 	mutex_lock(&resource->open_release);
 
! cocci
-	timeout = wait_event_interruptible_timeout(resource->twopc_wait,
-						   (r = inc_open_count(device, mode)),
-						   timeout);
+	wait_event_interruptible_timeout(timeout, resource->twopc_wait,
+					 (r = inc_open_count(device, mode)),
+					 timeout);
 
 	if (r == IOC_ABORT || (r == IOC_SLEEP && timeout <= 0)) {
 		mutex_unlock(&resource->open_release);
@@ -2701,7 +2738,7 @@
 		} else if ((mode & FMODE_NDELAY) == 0) {
 			/* Double check peers
 			 *
! remove
-			 * Some services may try to first open ro, and only if that
+			 * Some services may try_ to first open ro, and only if that
 			 * works open rw.  An attempt to failover immediately after
 			 * primary crash, before DRBD has noticed that the primary peer
 			 * is gone, would result in open failure, thus failure to take
@@ -2712,9 +2749,10 @@
 				err = -EAGAIN;
 			}
 			if (err == -EAGAIN) {
! cocci
-				wait_event_interruptible_timeout(resource->state_wait,
-					ro_open_cond(device) != -EAGAIN,
-					resource->res_opts.auto_promote_timeout * HZ / 10);
+				wait_event_interruptible_timeout(remaining_time,
+								 resource->state_wait,
+								 ro_open_cond(device) != -EAGAIN,
+								 resource->res_opts.auto_promote_timeout * HZ / 10);
 			}
 		}
 	} else if (resource->role[NOW] != R_PRIMARY &&
@@ -2766,10 +2804,17 @@
 	int open_rw_cnt, open_ro_cnt;
 
 	mutex_lock(&resource->open_release);
! remove: just for debugging
-	if (mode & FMODE_WRITE)
-		device->open_rw_cnt--;
-	else
-		device->open_ro_cnt--;
+	if (mode & FMODE_WRITE) {
+		if (device->open_rw_cnt > 0)
+			device->open_rw_cnt--;
+		else
+			printk("DRBD device already closed (device->open_rw_cnt is %d).\n", device->open_rw_cnt);
+	} else {
+		if (device->open_ro_cnt > 0)
+			device->open_ro_cnt--;
+		else
+			printk("DRBD device already closed (device->open_ro_cnt is %d).\n", device->open_ro_cnt);
+	}
 
 	drbd_open_counts(resource, &open_rw_cnt, &open_ro_cnt);
 
@@ -2845,7 +2890,6 @@
 	drbd_set_defaults(device);
 }
 
-
 static void drbd_destroy_mempools(void)
 {
 	bioset_exit(&drbd_io_bio_set);
@@ -2877,25 +2921,26 @@
 
 	/* caches */
 	drbd_request_cache = kmem_cache_create(
! remove
-		"drbd_req", sizeof(struct drbd_request), 0, 0, NULL);
+		"drbd_req", sizeof(struct drbd_request), 0, 0, NULL, '02WD');
 	if (drbd_request_cache == NULL)
 		goto Enomem;
 
 	drbd_ee_cache = kmem_cache_create(
! remove
-		"drbd_ee", sizeof(struct drbd_peer_request), 0, 0, NULL);
+		"drbd_ee", sizeof(struct drbd_peer_request), 0, 0, NULL, '03WD');
 	if (drbd_ee_cache == NULL)
 		goto Enomem;
 
 	drbd_bm_ext_cache = kmem_cache_create(
! remove
-		"drbd_bm", sizeof(struct bm_extent), 0, 0, NULL);
+		"drbd_bm", sizeof(struct bm_extent), 0, 0, NULL, '04WD');
 	if (drbd_bm_ext_cache == NULL)
 		goto Enomem;
 
 	drbd_al_ext_cache = kmem_cache_create(
! remove
-		"drbd_al", sizeof(struct lc_element), 0, 0, NULL);
+		"drbd_al", sizeof(struct lc_element), 0, 0, NULL, '05WD');
 	if (drbd_al_ext_cache == NULL)
 		goto Enomem;
 
! compat: biosets
+#if 0
 	/* mempools */
 	ret = bioset_init(&drbd_io_bio_set, BIO_POOL_SIZE, 0, 0);
 	if (ret)
@@ -2905,6 +2950,7 @@
 			  BIOSET_NEED_BVECS);
 	if (ret)
 		goto Enomem;
! compat: biosets
+#endif
 
 	ret = mempool_init_page_pool(&drbd_md_io_page_pool, DRBD_MIN_POOL_PAGES, 0);
 	if (ret)
@@ -2973,7 +3019,7 @@
 		free_peer_device(peer_device);
 	}
 
! review is this a bug?
-	__free_page(device->md_io.page);
+	put_page(device->md_io.page);
 	kref_debug_destroy(&device->kref_debug);
 
 	INIT_WORK(&device->finalize_work, drbd_device_finalize_work_fn);
@@ -2987,7 +3033,7 @@
 	while (resource->pp_pool) {
 		page = resource->pp_pool;
 		resource->pp_pool = page_chain_next(page);
! review is this a bug?
-		__free_page(page);
+		put_page(page);
 		resource->pp_vacant--;
 	}
 }
@@ -3012,7 +3058,7 @@
 
 	drbd_thread_stop_nowait(&resource->worker);
 
! cocci typeof
-	list_for_each_entry_safe(connection, tmp, &resource->twopc_parents, twopc_parent_list) {
+	list_for_each_entry_safe(struct drbd_connection, connection, tmp, &resource->twopc_parents, twopc_parent_list) {
 		kref_debug_put(&connection->kref_debug, 9);
 		kref_put(&connection->kref, drbd_destroy_connection);
 	}
@@ -3030,36 +3076,38 @@
 
 	spinlock_t lock;
 	struct list_head writes;
! review needed? this can probably be removed
+	struct task_struct task;
 } retry;
 
 static void drbd_req_destroy_lock(struct kref *kref)
 {
! cocci
+	KIRQL spin_lock_irq_flags;
 	struct drbd_request *req = container_of(kref, struct drbd_request, kref);
 	struct drbd_resource *resource = req->device->resource;
 
! cocci
-	spin_lock_irq(&resource->req_lock);
+	spin_lock_irqsave(&resource->req_lock, spin_lock_irq_flags);
 	drbd_req_destroy(kref);
! cocci
-	spin_unlock_irq(&resource->req_lock);
+	spin_unlock_irqrestore(&resource->req_lock, spin_lock_irq_flags);
 }
 
 static void do_retry(struct work_struct *ws)
 {
! cocci
+	KIRQL spin_lock_irq_flags;
 	struct retry_worker *retry = container_of(ws, struct retry_worker, worker);
 	LIST_HEAD(writes);
 	struct drbd_request *req, *tmp;
 
! cocci
-	spin_lock_irq(&retry->lock);
+	spin_lock_irqsave(&retry->lock, spin_lock_irq_flags);
 	list_splice_init(&retry->writes, &writes);
! cocci
-	spin_unlock_irq(&retry->lock);
+	spin_unlock_irqrestore(&retry->lock, spin_lock_irq_flags);
 
! cocci
-	list_for_each_entry_safe(req, tmp, &writes, tl_requests) {
+	list_for_each_entry_safe(struct drbd_request, req, tmp, &writes, tl_requests) {
 		struct drbd_device *device = req->device;
 		struct bio *bio = req->master_bio;
! cocci
-		unsigned long start_jif = req->start_jif;
+		ULONG_PTR start_jif = req->start_jif;
 		bool expected;
 		ktime_get_accounting_assign(ktime_t start_kt, req->start_kt);
 
-
 		expected =
 			expect(device, atomic_read(&req->completion_ref) == 0) &&
 			expect(device, req->local_rq_state & RQ_POSTPONED) &&
@@ -3099,7 +3147,7 @@
  * holds resource->req_lock */
 void drbd_restart_request(struct drbd_request *req)
 {
! cocci
-	unsigned long flags;
+	KIRQL flags;
 	spin_lock_irqsave(&retry.lock, flags);
 	list_move_tail(&req->tl_requests, &retry.writes);
 	spin_unlock_irqrestore(&retry.lock, flags);
@@ -3112,8 +3160,7 @@
 	queue_work(retry.wq, &retry.worker);
 }
 
-
! compat: call it via the module struct (module_exit()), then remove
-static void drbd_cleanup(void)
+void drbd_cleanup(void)
 {
 	/* first remove proc,
 	 * drbdsetup uses its presence to detect
@@ -3163,7 +3210,7 @@
 
 void drbd_queue_work(struct drbd_work_queue *q, struct drbd_work *w)
 {
! cocci
-	unsigned long flags;
+	KIRQL flags;
 
 	spin_lock_irqsave(&q->q_lock, flags);
 	list_add_tail(&w->list, &q->q);
@@ -3183,12 +3230,13 @@
 
 struct drbd_resource *drbd_find_resource(const char *name)
 {
! cocci
+	KIRQL rcu_flags;
 	struct drbd_resource *resource;
 
 	if (!name || !name[0])
 		return NULL;
 
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_resource_rcu(resource, &drbd_resources) {
 		if (!strcmp(resource->name, name)) {
 			kref_get(&resource->kref);
@@ -3197,7 +3245,7 @@
 	}
 	resource = NULL;
 found:
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 	return resource;
 }
 
@@ -3235,18 +3283,20 @@
 
 void drbd_flush_peer_acks(struct drbd_resource *resource)
 {
! cocci
-	spin_lock_irq(&resource->req_lock);
+	KIRQL flags;
+
+	spin_lock_irqsave(&resource->req_lock, flags);
 	if (resource->peer_ack_req) {
 		resource->last_peer_acked_dagtag = resource->peer_ack_req->dagtag_sector;
 		drbd_queue_peer_ack(resource, resource->peer_ack_req);
 		resource->peer_ack_req = NULL;
 	}
! cocci
-	spin_unlock_irq(&resource->req_lock);
+	spin_unlock_irqrestore(&resource->req_lock, flags);
 }
 
 static void peer_ack_timer_fn(struct timer_list *t)
 {
! cocci
-	struct drbd_resource *resource = from_timer(resource, t, peer_ack_timer);
+	struct drbd_resource *resource = from_timer(resource, t, peer_ack_timer, struct drbd_resource);
 
 	drbd_flush_peer_acks(resource);
 }
@@ -3272,22 +3322,24 @@
 
 static void wake_all_device_misc(struct drbd_resource *resource)
 {
! cocci
+	KIRQL rcu_flags;
 	struct drbd_device *device;
 	int vnr;
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	idr_for_each_entry(&resource->devices, device, vnr)
 		wake_up(&device->misc_wait);
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 }
 
 int set_resource_options(struct drbd_resource *resource, struct res_opts *res_opts)
 {
! compat: need to have this
+#if 0
 	struct drbd_connection *connection;
 	cpumask_var_t new_cpu_mask;
 	int err;
 	bool wake_device_misc = false;
 	bool force_state_recalc = false;
-	unsigned long irq_flags;
+	ULONG_PTR irq_flags;
 	struct res_opts *old_opts = &resource->res_opts;
 
 	if (!zalloc_cpumask_var(&new_cpu_mask, GFP_KERNEL))
@@ -3373,7 +3425,10 @@
 fail:
 	free_cpumask_var(new_cpu_mask);
 	return err;
-
! compat: need to have this
+#else
+	resource->res_opts = *res_opts;
+	return 0;
+#endif
 }
 
 struct drbd_resource *drbd_create_resource(const char *name,
@@ -3384,7 +3439,7 @@
 	const int page_pool_count = DRBD_MAX_BIO_SIZE/PAGE_SIZE;
 	int i;
 
! remove
-	resource = kzalloc(sizeof(struct drbd_resource), GFP_KERNEL);
+	resource = kzalloc(sizeof(struct drbd_resource), GFP_KERNEL, '06WD');
 	if (!resource)
 		goto fail;
 	resource->name = kstrdup(name, GFP_KERNEL);
@@ -3461,14 +3516,14 @@
 	int size;
 
 	size = sizeof(*connection) - sizeof(connection->transport) + tc->instance_size;
! remove
-	connection = kzalloc(size, GFP_KERNEL);
+	connection = kzalloc(size, GFP_KERNEL, '07WD');
 	if (!connection)
 		return NULL;
 
 	if (drbd_alloc_send_buffers(connection))
 		goto fail;
 
! remove
-	connection->current_epoch = kzalloc(sizeof(struct drbd_epoch), GFP_KERNEL);
+	connection->current_epoch = kzalloc(sizeof(struct drbd_epoch), GFP_KERNEL, '08WD');
 	if (!connection->current_epoch)
 		goto fail;
 
@@ -3593,7 +3648,7 @@
 	struct drbd_peer_device *peer_device;
 	int err;
 
! remove
-	peer_device = kzalloc(sizeof(struct drbd_peer_device), GFP_KERNEL);
+	peer_device = kzalloc(sizeof(struct drbd_peer_device), GFP_KERNEL, '09WD');
 	if (!peer_device)
 		return NULL;
 
@@ -3649,6 +3704,7 @@
 enum drbd_ret_code drbd_create_device(struct drbd_config_context *adm_ctx, unsigned int minor,
 				      struct device_conf *device_conf, struct drbd_device **p_device)
 {
! cocci
+	KIRQL spin_lock_flags;
 	struct drbd_resource *resource = adm_ctx->resource;
 	struct drbd_connection *connection;
 	struct drbd_device *device;
@@ -3660,15 +3716,23 @@
 	int vnr = adm_ctx->volume;
 	enum drbd_ret_code err = ERR_NOMEM;
 	bool locked = false;
! compat: make this (creating a block device) more Linux compatible
+	struct block_device *block_device;
+	KIRQL spin_lock_irq_flags;
+	KIRQL req_lock_flags;
 
 	device = minor_to_device(minor);
 	if (device)
 		return ERR_MINOR_OR_VOLUME_EXISTS;
 
! compat: make this (creating a block device) more Linux compatible
+	block_device = bdget(MKDEV(DRBD_MAJOR, minor));
+	if (block_device == NULL)
+		return ERR_NO_DISK;
+
 	/* GFP_KERNEL, we are outside of all write-out paths */
-	device = kzalloc(sizeof(struct drbd_device), GFP_KERNEL);
+	device = kzalloc(sizeof(struct drbd_device), GFP_KERNEL, '0AWD');
 	if (!device)
! compat: make this (creating a block device) more Linux compatible
-		return ERR_NOMEM;
+		goto out_no_device;
+
 	kref_init(&device->kref);
 	kref_debug_init(&device->kref_debug, &device->kref, &kref_class_device);
 
@@ -3722,6 +3786,15 @@
 	if (!disk)
 		goto out_no_disk;
 
! compat: make this (creating a block device) more Linux compatible
+        device->this_bdev = block_device;
+	if (block_device->bd_disk != NULL) {
+		printk("Warning: block_device already has a disk, freeing it.\n");
+		if (block_device->bd_disk->queue)
+			blk_cleanup_queue(block_device->bd_disk->queue);
+		put_disk(block_device->bd_disk);
+	}
+        device->this_bdev->bd_disk = disk;
+
 	device->vdisk = disk;
 	device->rq_queue = disk->queue;
 
@@ -3731,6 +3804,8 @@
 	disk->fops = &drbd_ops;
 	sprintf(disk->disk_name, "drbd%d", minor);
 	disk->private_data = device;
! compat: make this (creating a block device) more Linux compatible
+	disk->part0 = block_device;
+	disk->bdev = block_device;
 
 	blk_queue_flag_set(QUEUE_FLAG_STABLE_WRITES, disk->queue);
 	blk_queue_write_cache(disk->queue, true, true);
@@ -3766,10 +3841,10 @@
 	INIT_LIST_HEAD(&device->pending_bitmap_io);
 
 	locked = true;
! cocci
-	spin_lock_irq(&resource->req_lock);
-	spin_lock(&drbd_devices_lock);
+	spin_lock_irqsave(&resource->req_lock, req_lock_flags);
+	spin_lock_irqsave(&drbd_devices_lock, spin_lock_flags);
 	id = idr_alloc(&drbd_devices, device, minor, minor + 1, GFP_NOWAIT);
-	spin_unlock(&drbd_devices_lock);
+	spin_unlock_irqrestore(&drbd_devices_lock, spin_lock_flags);
 	if (id < 0) {
 		if (id == -ENOSPC)
 			err = ERR_MINOR_OR_VOLUME_EXISTS;
@@ -3787,7 +3862,7 @@
 	kref_get(&device->kref);
 	kref_debug_get(&device->kref_debug, 1);
 
! cocci
-	list_for_each_entry_safe(peer_device, tmp_peer_device, &peer_devices, peer_devices) {
+	list_for_each_entry_safe(struct drbd_peer_device, peer_device, tmp_peer_device, &peer_devices, peer_devices) {
 		connection = peer_device->connection;
 		id = idr_alloc(&connection->peer_devices, peer_device,
 			       device->vnr, device->vnr + 1, GFP_NOWAIT);
@@ -3800,7 +3875,7 @@
 		kref_get(&device->kref);
 		kref_debug_get(&device->kref_debug, 1);
 	}
! cocci
-	spin_unlock_irq(&resource->req_lock);
+	spin_unlock_irqrestore(&resource->req_lock, req_lock_flags);
 	locked = false;
 
 	if (init_submitter(device)) {
@@ -3823,13 +3898,22 @@
 
 	drbd_debugfs_device_add(device);
 	*p_device = device;
! compat: make this (creating a block device) more Linux compatible
+
+		/* From here we consider the DRBD device as valid.
+		 * drbd_open and I/O will be called.
+		 */
+	device->this_bdev->drbd_device = device;
+
+		/* Tell the PnP manager that we are there ... */
! compat: call this when drbd_create_device returned with NO_ERROR
+	windrbd_rescan_bus();
+
 	return NO_ERROR;
 
 out_remove_peer_device:
 	list_add_rcu(&tmp, &device->peer_devices);
 	list_del_init(&device->peer_devices);
 	synchronize_rcu();
! cocci
-	list_for_each_entry_safe(peer_device, tmp_peer_device, &tmp, peer_devices) {
+	list_for_each_entry_safe(struct drbd_peer_device, peer_device, tmp_peer_device, &tmp, peer_devices) {
 		struct drbd_connection *connection = peer_device->connection;
 
 		idr_remove(&connection->peer_devices, device->vnr);
@@ -3842,25 +3926,27 @@
 	idr_remove(&resource->devices, vnr);
 	kref_debug_put(&device->kref_debug, 1);
 
! compat: make this (creating a block device) more Linux compatible
+	idr_remove(&resource->devices, vnr);
+
 out_idr_remove_minor:
! cocci
-	spin_lock(&drbd_devices_lock);
+	spin_lock_irqsave(&drbd_devices_lock, spin_lock_flags);
 	idr_remove(&drbd_devices, minor);
! cocci
-	spin_unlock(&drbd_devices_lock);
+	spin_unlock_irqrestore(&drbd_devices_lock, spin_lock_flags);
 	kref_debug_put(&device->kref_debug, 1);
 out_no_minor_idr:
 	if (locked)
! cocci
-		spin_unlock_irq(&resource->req_lock);
+		spin_unlock_irqrestore(&resource->req_lock, req_lock_flags);
 	synchronize_rcu();
 
 out_no_peer_device:
! cocci
-	list_for_each_entry_safe(peer_device, tmp_peer_device, &peer_devices, peer_devices) {
+	list_for_each_entry_safe(struct drbd_peer_device, peer_device, tmp_peer_device, &peer_devices, peer_devices) {
 		list_del(&peer_device->peer_devices);
 		kfree(peer_device);
 	}
 
 	drbd_bm_free(device->bitmap);
 out_no_bitmap:
! review: is this a bug?
-	__free_page(device->md_io.page);
+	put_page(device->md_io.page);
 out_no_io_page:
 	blk_cleanup_disk(disk);
 out_no_disk:
@@ -3870,6 +3956,10 @@
 	kref_debug_put(&device->kref_debug, 4);
 	kref_debug_destroy(&device->kref_debug);
 	kfree(device);
! compat
+out_no_device:
+	bdput(block_device);	/* This will also remove the symbolic link
+				   (mount point) if present. */
+
 	return err;
 }
 
@@ -3882,19 +3972,21 @@
  */
 void drbd_unregister_device(struct drbd_device *device)
 {
! cocci
+	KIRQL spin_lock_flags;
+	KIRQL spin_lock_irq_flags;
 	struct drbd_resource *resource = device->resource;
 	struct drbd_connection *connection;
 	struct drbd_peer_device *peer_device;
 
! cocci
-	spin_lock_irq(&resource->req_lock);
+	spin_lock_irqsave(&resource->req_lock, spin_lock_irq_flags);
 	for_each_connection(connection, resource) {
 		idr_remove(&connection->peer_devices, device->vnr);
 	}
 	idr_remove(&resource->devices, device->vnr);
! cocci
-	spin_lock(&drbd_devices_lock);
+	spin_lock_irqsave(&drbd_devices_lock, spin_lock_flags);
 	idr_remove(&drbd_devices, device->minor);
! cocci
-	spin_unlock(&drbd_devices_lock);
-	spin_unlock_irq(&resource->req_lock);
+	spin_unlock_irqrestore(&drbd_devices_lock, spin_lock_flags);
+	spin_unlock_irqrestore(&resource->req_lock, spin_lock_irq_flags);
 
 	for_each_peer_device(peer_device, device)
 		drbd_debugfs_peer_device_cleanup(peer_device);
@@ -3904,6 +3996,10 @@
 	destroy_workqueue(device->submit.wq);
 	device->submit.wq = NULL;
 	del_timer_sync(&device->request_timer);
! compat (bpput, ...)
+
+		/* TODO: Ask phil if that is correct */
+	bdput(device->this_bdev);
+	device->this_bdev = NULL;
 }
 
 void drbd_reclaim_device(struct rcu_head *rp)
@@ -3931,12 +4027,13 @@
  */
 void drbd_unregister_connection(struct drbd_connection *connection)
 {
! cocci
+	KIRQL spin_lock_irq_flags;
 	struct drbd_resource *resource = connection->resource;
 	struct drbd_peer_device *peer_device;
 	LIST_HEAD(work_list);
 	int vnr, rr;
 
! cocci
-	spin_lock_irq(&resource->req_lock);
+	spin_lock_irqsave(&resource->req_lock, spin_lock_irq_flags);
 	set_bit(C_UNREGISTERED, &connection->flags);
 	smp_wmb();
 	idr_for_each_entry(&connection->peer_devices, peer_device, vnr) {
@@ -3944,9 +4041,9 @@
 		list_add(&peer_device->peer_devices, &work_list);
 	}
 	list_del_rcu(&connection->connections);
! cocci
-	spin_unlock_irq(&resource->req_lock);
+	spin_unlock_irqrestore(&resource->req_lock, spin_lock_irq_flags);
 
-	list_for_each_entry(peer_device, &work_list, peer_devices)
+	list_for_each_entry(struct drbd_peer_device, peer_device, &work_list, peer_devices)
 		drbd_debugfs_peer_device_cleanup(peer_device);
 	drbd_debugfs_connection_cleanup(connection);
 
@@ -3988,7 +4085,7 @@
 	kref_put(&connection->kref, drbd_destroy_connection);
 }
 
! compat: call via module_init()
-static int __init drbd_init(void)
+int __init drbd_init(void)
 {
 	int err;
 
@@ -4030,15 +4127,8 @@
 		goto fail;
 
 	err = -ENOMEM;
! compat: implement /proc
-	drbd_proc = proc_create_single("drbd", S_IFREG | 0444 , NULL,
-			drbd_seq_show);
-
-	if (!drbd_proc)	{
-		pr_err("unable to register proc file\n");
-		goto fail;
-	}
 
-	retry.wq = create_singlethread_workqueue("drbd-reissue");
+	retry.wq = alloc_ordered_workqueue("drbd-reissue", 0);
 	if (!retry.wq) {
 		pr_err("unable to create retry workqueue\n");
 		goto fail;
@@ -4047,7 +4137,12 @@
 	spin_lock_init(&retry.lock);
 	INIT_LIST_HEAD(&retry.writes);
 
! compat: fix DEFINE_SPINLOCK - or maybe manual - or compat (do this once drbd_init returns)
+	spin_lock_init(&drbd_devices_lock);
+	mutex_init(&resources_mutex);
+	mutex_init(&notification_mutex);	/* defined in drbd_nl.c */
! compat: implement debugfs
+#if 0
 	drbd_debugfs_init();
+#endif
 
 	pr_info("initialized. "
 	       "Version: " REL_VERSION " (api:%d/proto:%d-%d)\n",
@@ -4193,7 +4288,7 @@
 		mod_timer(&device->md_sync_timer, jiffies + 5*HZ);
 }
 
! header: define __must_hold
-void _drbd_uuid_push_history(struct drbd_device *device, u64 val) __must_hold(local)
+void _drbd_uuid_push_history(struct drbd_device *device, u64 val) 
 {
 	struct drbd_md *md = &device->ldev->md;
 	int node_id, i;
@@ -4223,7 +4318,7 @@
 	md->history_uuids[i] = val;
 }
 
! header: define __must_hold
-u64 _drbd_uuid_pull_history(struct drbd_peer_device *peer_device) __must_hold(local)
+u64 _drbd_uuid_pull_history(struct drbd_peer_device *peer_device) 
 {
 	struct drbd_device *device = peer_device->device;
 	struct drbd_md *md = &device->ldev->md;
@@ -4260,19 +4355,19 @@
 	peer_md->bitmap_dagtag = val ? device->resource->dagtag_sector : 0;
 }
 
! header: define __must_hold
-void _drbd_uuid_set_current(struct drbd_device *device, u64 val) __must_hold(local)
+void _drbd_uuid_set_current(struct drbd_device *device, u64 val) 
 {
! cocci
-	unsigned long flags;
+	KIRQL flags;
 
 	spin_lock_irqsave(&device->ldev->md.uuid_lock, flags);
 	__drbd_uuid_set_current(device, val);
 	spin_unlock_irqrestore(&device->ldev->md.uuid_lock, flags);
 }
 
! header: define __must_hold
-void _drbd_uuid_set_bitmap(struct drbd_peer_device *peer_device, u64 val) __must_hold(local)
+void _drbd_uuid_set_bitmap(struct drbd_peer_device *peer_device, u64 val) 
 {
 	struct drbd_device *device = peer_device->device;
-	unsigned long flags;
! cocci
+	KIRQL flags;
 
 	down_write(&device->uuid_sem);
 	spin_lock_irqsave(&device->ldev->md.uuid_lock, flags);
@@ -4282,10 +4377,10 @@
 }
 
 /* call holding down_write(uuid_sem) */
! header: define __must_hold
-void drbd_uuid_set_bitmap(struct drbd_peer_device *peer_device, u64 uuid) __must_hold(local)
+void drbd_uuid_set_bitmap(struct drbd_peer_device *peer_device, u64 uuid) 
 {
 	struct drbd_device *device = peer_device->device;
! cocci
-	unsigned long flags;
+	KIRQL flags;
 	u64 previous_uuid;
 
 	spin_lock_irqsave(&device->ldev->md.uuid_lock, flags);
@@ -4296,8 +4391,9 @@
 	spin_unlock_irqrestore(&device->ldev->md.uuid_lock, flags);
 }
 
! header: define __must_hold
-static u64 rotate_current_into_bitmap(struct drbd_device *device, u64 weak_nodes, u64 dagtag) __must_hold(local)
+static u64 rotate_current_into_bitmap(struct drbd_device *device, u64 weak_nodes, u64 dagtag) 
 {
! cocci
+	KIRQL rcu_flags;
 	struct drbd_peer_md *peer_md = device->ldev->md.peers;
 	struct drbd_peer_device *peer_device;
 	int node_id;
@@ -4313,7 +4409,7 @@
 	else
 		get_random_bytes(&prev_c_uuid, sizeof(u64));
 
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_peer_device_rcu(peer_device, device) {
 		enum drbd_disk_state pdsk;
 		node_id = peer_device->node_id;
@@ -4325,7 +4421,7 @@
 			continue;
 		}
 		node_mask |= NODE_MASK(node_id);
! cocci
-		__set_bit(peer_device->bitmap_index, (unsigned long*)&slot_mask);
+		__set_bit(peer_device->bitmap_index, (ULONG_PTR*)&slot_mask);
 		bm_uuid = peer_md[node_id].bitmap_uuid;
 		if (bm_uuid && bm_uuid != prev_c_uuid)
 			continue;
@@ -4347,16 +4443,16 @@
 			continue;
 		slot_nr = peer_md[node_id].bitmap_index;
 		if (slot_nr != -1) {
! cocci
-			if (test_bit(slot_nr, (unsigned long*)&slot_mask))
+			if (test_bit(slot_nr, (ULONG_PTR*)&slot_mask))
 				continue;
! cocci
-			__set_bit(slot_nr, (unsigned long*)&slot_mask);
+			__set_bit(slot_nr, (ULONG_PTR*)&slot_mask);
 		}
 		bm_uuid = peer_md[node_id].bitmap_uuid;
 		if (bm_uuid && bm_uuid != prev_c_uuid)
 			continue;
 		if (slot_nr == -1) {
! cocci
-			slot_nr = find_first_zero_bit((unsigned long*)&slot_mask, sizeof(slot_mask) * BITS_PER_BYTE);
-			__set_bit(slot_nr, (unsigned long*)&slot_mask);
+			slot_nr = find_first_zero_bit((ULONG_PTR*)&slot_mask, sizeof(slot_mask) * BITS_PER_BYTE);
+			__set_bit(slot_nr, (ULONG_PTR*)&slot_mask);
 		}
 		peer_md[node_id].bitmap_uuid = prev_c_uuid;
 		peer_md[node_id].bitmap_dagtag = dagtag;
@@ -4365,7 +4461,7 @@
 		if (slot_nr < device->bitmap->bm_max_peers)
 			got_new_bitmap_uuid |= NODE_MASK(node_id);
 	}
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return got_new_bitmap_uuid;
 }
@@ -4386,44 +4482,47 @@
 
 u64 drbd_weak_nodes_device(struct drbd_device *device)
 {
! cocci
+	KIRQL rcu_flags;
 	struct drbd_peer_device *peer_device;
 	u64 not_weak = 0;
 
 	if (device->disk_state[NOW] == D_UP_TO_DATE)
 		not_weak = NODE_MASK(device->resource->res_opts.node_id);
 
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_peer_device_rcu(peer_device, device) {
 		enum drbd_disk_state pdsk = peer_device->disk_state[NOW];
 		if (!(pdsk <= D_FAILED || pdsk == D_UNKNOWN || pdsk == D_OUTDATED))
 			not_weak |= NODE_MASK(peer_device->node_id);
 
 	}
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return ~not_weak;
 }
 
-
! header: define __must_hold
-static bool __new_current_uuid_prepare(struct drbd_device *device, bool forced) __must_hold(local)
+static bool __new_current_uuid_prepare(struct drbd_device *device, bool forced) 
 {
! cocci
+	KIRQL spin_lock_irq_flags;
 	u64 got_new_bitmap_uuid, val, old_current_uuid;
 	int err;
 
! cocci
-	spin_lock_irq(&device->ldev->md.uuid_lock);
+	spin_lock_irqsave(&device->ldev->md.uuid_lock, spin_lock_irq_flags);
 	got_new_bitmap_uuid = rotate_current_into_bitmap(device,
 					forced ? initial_resync_nodes(device) : 0,
 					device->resource->dagtag_sector);
 
 	if (!got_new_bitmap_uuid) {
! cocci
-		spin_unlock_irq(&device->ldev->md.uuid_lock);
+		spin_unlock_irqrestore(&device->ldev->md.uuid_lock,
+				       spin_lock_irq_flags);
 		return false;
 	}
 
 	old_current_uuid = device->ldev->md.current_uuid;
 	get_random_bytes(&val, sizeof(u64));
 	__drbd_uuid_set_current(device, val);
! cocci
-	spin_unlock_irq(&device->ldev->md.uuid_lock);
+	spin_unlock_irqrestore(&device->ldev->md.uuid_lock,
+			       spin_lock_irq_flags);
 
 	/* get it to stable storage _now_ */
 	err = drbd_md_sync(device);
@@ -4441,7 +4540,7 @@
 		  device->ldev->md.current_uuid, weak_nodes);
 }
 
! header: define __must_hold
-static void __new_current_uuid_send(struct drbd_device *device, u64 weak_nodes, bool forced) __must_hold(local)
+static void __new_current_uuid_send(struct drbd_device *device, u64 weak_nodes, bool forced) 
 {
 	struct drbd_peer_device *peer_device;
 	u64 im;
@@ -4452,7 +4551,7 @@
 	}
 }

! header: define __must_hold
-static void __drbd_uuid_new_current_send(struct drbd_device *device, bool forced) __must_hold(local)
+static void __drbd_uuid_new_current_send(struct drbd_device *device, bool forced) 
 {
 	u64 weak_nodes;
 
@@ -4468,7 +4567,7 @@
 	up_read(&device->uuid_sem);
 }
 
 
! header: define __must_hold
-static void __drbd_uuid_new_current_holding_uuid_sem(struct drbd_device *device) __must_hold(local)
+static void __drbd_uuid_new_current_holding_uuid_sem(struct drbd_device *device) 
 {
 	u64 weak_nodes;
 
@@ -4506,10 +4605,11 @@
 
 static bool diskfull_peers_need_new_cur_uuid(struct drbd_device *device)
 {
! cocci
+	KIRQL rcu_flags;
 	struct drbd_peer_device *peer_device;
 	bool rv = false;
 
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_peer_device_rcu(peer_device, device) {
 		/* Only an up-to-date peer persists a new current uuid! */
 		if (peer_device->disk_state[NOW] < D_UP_TO_DATE)
@@ -4519,17 +4619,18 @@
 			break;
 		}
 	}
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return rv;
 }
 
 static bool a_lost_peer_is_on_same_cur_uuid(struct drbd_device *device)
 {
! cocci
+	KIRQL rcu_flags;
 	struct drbd_peer_device *peer_device;
 	bool rv = false;
 
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_peer_device_rcu(peer_device, device) {
 		enum drbd_disk_state pdsk = peer_device->disk_state[NOW];
 
@@ -4541,7 +4642,7 @@
 			break;
 		}
 	}
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return rv;
 }
@@ -4598,9 +4699,10 @@
 
 static void drbd_propagate_uuids(struct drbd_device *device, u64 nodes)
 {
! cocci
+	KIRQL rcu_flags;
 	struct drbd_peer_device *peer_device;
 
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_peer_device_rcu(peer_device, device) {
 		if (!(nodes & NODE_MASK(peer_device->node_id)))
 			continue;
@@ -4612,11 +4714,12 @@
 			drbd_queue_work(&peer_device->connection->sender_work,
 					&peer_device->propagate_uuids_work);
 	}
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 }
 
! header
-void drbd_uuid_received_new_current(struct drbd_peer_device *from_pd, u64 val, u64 weak_nodes) __must_hold(local)
+void drbd_uuid_received_new_current(struct drbd_peer_device *from_pd, u64 val, u64 weak_nodes) 
 {
! cocci
+	KIRQL spin_lock_irq_flags;
 	struct drbd_device *device = from_pd->device;
 	u64 dagtag = from_pd->connection->last_dagtag_sector;
 	struct drbd_peer_device *peer_device;
@@ -4624,7 +4727,7 @@
 	bool set_current = true;
 
 	down_write(&device->uuid_sem);
! cocci
-	spin_lock_irq(&device->ldev->md.uuid_lock);
+	spin_lock_irqsave(&device->ldev->md.uuid_lock, spin_lock_irq_flags);
 
 	for_each_peer_device(peer_device, device) {
 		if (peer_device->repl_state[NOW] == L_SYNC_TARGET ||
@@ -4668,14 +4771,15 @@
 		_drbd_uuid_push_history(device, old_current);
 	}
 
! cocci
-	spin_unlock_irq(&device->ldev->md.uuid_lock);
+	spin_unlock_irqrestore(&device->ldev->md.uuid_lock,
+			       spin_lock_irq_flags);
 	downgrade_write(&device->uuid_sem);
 	if (set_current)
 		drbd_propagate_uuids(device, receipients);
 	up_read(&device->uuid_sem);
 }
 
! header
-static u64 __set_bitmap_slots(struct drbd_device *device, u64 bitmap_uuid, u64 do_nodes) __must_hold(local)
+static u64 __set_bitmap_slots(struct drbd_device *device, u64 bitmap_uuid, u64 do_nodes) 
 {
 	struct drbd_peer_md *peer_md = device->ldev->md.peers;
 	u64 modified = 0;
@@ -4703,7 +4807,7 @@
 	return modified;
 }
 
! header
-static u64 __test_bitmap_slots(struct drbd_device *device) __must_hold(local)
+static u64 __test_bitmap_slots(struct drbd_device *device) 
 {
 	struct drbd_peer_md *peer_md = device->ldev->md.peers;
 	int node_id;
@@ -4721,7 +4825,7 @@
    SyncSource had. It might be that in the mean time some peers sent more
    recent UUIDs to me. Remove all peers that are on the same UUID as I am
    now from the set of nodes */
! header
-static u64 __test_bitmap_slots_of_peer(struct drbd_peer_device *peer_device) __must_hold(local)
+static u64 __test_bitmap_slots_of_peer(struct drbd_peer_device *peer_device) 
 {
 	u64 set_bitmap_slots = 0;
 	int node_id;
@@ -4739,11 +4843,12 @@
 static u64
 peers_with_current_uuid(struct drbd_device *device, u64 current_uuid)
 {
! cocci
+	KIRQL rcu_flags;
 	struct drbd_peer_device *peer_device;
 	u64 nodes = 0;
 
 	current_uuid &= ~UUID_PRIMARY;
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_peer_device_rcu(peer_device, device) {
 		enum drbd_disk_state peer_disk_state = peer_device->disk_state[NOW];
 		if (peer_disk_state < D_INCONSISTENT || peer_disk_state == D_UNKNOWN)
@@ -4751,12 +4856,12 @@
 		if (current_uuid == (peer_device->current_uuid & ~UUID_PRIMARY))
 			nodes |= NODE_MASK(peer_device->node_id);
 	}
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return nodes;
 }
 
! header
-void drbd_uuid_resync_starting(struct drbd_peer_device *peer_device) __must_hold(local)
+void drbd_uuid_resync_starting(struct drbd_peer_device *peer_device) 
 {
 	struct drbd_device *device = peer_device->device;
 
@@ -4766,10 +4871,10 @@
 	rotate_current_into_bitmap(device, false, device->resource->dagtag_sector);
 }
! header
 
-u64 drbd_uuid_resync_finished(struct drbd_peer_device *peer_device) __must_hold(local)
+u64 drbd_uuid_resync_finished(struct drbd_peer_device *peer_device) 
 {
 	struct drbd_device *device = peer_device->device;
! cocci
-	unsigned long flags;
+	KIRQL flags;
 	u64 ss_nz_bm; /* sync_source has non zero bitmap for. expressed as nodemask */
 	u64 pwcu; /* peers with current uuid */
 	u64 newer;
@@ -4795,31 +4900,33 @@
 	return connection ? rcu_dereference(connection->transport.net_conf)->name : "";
 }
 
! manual (spinlock flags passed between functions)
-static void forget_bitmap(struct drbd_device *device, int node_id) __must_hold(local)
+static void forget_bitmap(struct drbd_device *device, int node_id, KIRQL *spin_lock_flags_p)
 {
! cocci
+	KIRQL rcu_flags;
 	int bitmap_index = device->ldev->md.peers[node_id].bitmap_index;
 	const char* name;
 
 	if (_drbd_bm_total_weight(device, bitmap_index) == 0)
 		return;
 
! manual (spinlock flags passed between functions)
-	spin_unlock_irq(&device->ldev->md.uuid_lock);
-	rcu_read_lock();
+	spin_unlock_irqrestore(&device->ldev->md.uuid_lock, *spin_lock_flags_p);
+	rcu_flags = rcu_read_lock();
 	name = name_of_node_id(device->resource, node_id);
 	drbd_info(device, "clearing bitmap UUID and content (%lu bits) for node %d (%s)(slot %d)\n",
 		  _drbd_bm_total_weight(device, bitmap_index), node_id, name, bitmap_index);
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 	drbd_suspend_io(device, WRITE_ONLY);
 	drbd_bm_lock(device, "forget_bitmap()", BM_LOCK_TEST | BM_LOCK_SET);
! cocci
-	_drbd_bm_clear_many_bits(device, bitmap_index, 0, -1UL);
+	_drbd_bm_clear_many_bits(device, bitmap_index, 0, ((ULONG_PTR)-1));
 	drbd_bm_unlock(device);
 	drbd_resume_io(device);
 	drbd_md_mark_dirty(device);
! manual (spinlock flags passed between functions)
-	spin_lock_irq(&device->ldev->md.uuid_lock);
+	spin_lock_irqsave(&device->ldev->md.uuid_lock, *spin_lock_flags_p);
 }
 
! header
-static void copy_bitmap(struct drbd_device *device, int from_id, int to_id) __must_hold(local)
+static void copy_bitmap(struct drbd_device *device, int from_id, int to_id, KIRQL *spin_lock_irq_flags_p)
 {
! cocci
+	KIRQL rcu_flags;
 	struct drbd_peer_device *peer_device = peer_device_by_node_id(device, to_id);
 	struct drbd_peer_md *peer_md = device->ldev->md.peers;
 	u64 previous_bitmap_uuid = peer_md[to_id].bitmap_uuid;
@@ -4836,23 +4943,23 @@
 	if (peer_device && peer_device->comm_bitmap_uuid == previous_bitmap_uuid)
 		peer_device->comm_bitmap_uuid = peer_md[from_id].bitmap_uuid;
 
-	spin_unlock_irq(&device->ldev->md.uuid_lock);
-	rcu_read_lock();
! manual (spinlock flags passed between functions)
+	spin_unlock_irqrestore(&device->ldev->md.uuid_lock, *spin_lock_irq_flags_p);
+	rcu_flags = rcu_read_lock();
 	from_name = name_of_node_id(device->resource, from_id);
 	to_name = name_of_node_id(device->resource, to_id);
 	drbd_info(device, "Node %d (%s) synced up to node %d (%s). copying bitmap slot %d to %d.\n",
 		  to_id, to_name, from_id, from_name, from_index, to_index);
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 	drbd_suspend_io(device, WRITE_ONLY);
 	drbd_bm_lock(device, "copy_bitmap()", BM_LOCK_ALL);
 	drbd_bm_copy_slot(device, from_index, to_index);
 	drbd_bm_unlock(device);
 	drbd_resume_io(device);
 	drbd_md_mark_dirty(device);
! manual (spinlock flags passed between functions)
-	spin_lock_irq(&device->ldev->md.uuid_lock);
+	spin_lock_irqsave(&device->ldev->md.uuid_lock, *spin_lock_irq_flags_p);
 }
 
! header
-static int find_node_id_by_bitmap_uuid(struct drbd_device *device, u64 bm_uuid) __must_hold(local)
+static int find_node_id_by_bitmap_uuid(struct drbd_device *device, u64 bm_uuid) 
 {
 	struct drbd_peer_md *peer_md = device->ldev->md.peers;
 	int node_id;
@@ -4875,19 +4982,20 @@
 
 static bool node_connected(struct drbd_resource *resource, int node_id)
 {
! cocci
+	KIRQL rcu_flags;
 	struct drbd_connection *connection;
 	bool r = false;
 
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	connection = drbd_connection_by_node_id(resource, node_id);
 	if (connection)
 		r = connection->cstate[NOW] == C_CONNECTED;
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return r;
 }

! manual
-static bool detect_copy_ops_on_peer(struct drbd_peer_device *peer_device) __must_hold(local)
+static bool detect_copy_ops_on_peer(struct drbd_peer_device *peer_device, KIRQL *spin_lock_irq_flags_p)
 {
 	struct drbd_device *device = peer_device->device;
 	struct drbd_peer_md *peer_md = device->ldev->md.peers;
@@ -4941,21 +5049,22 @@
 
 	if (from_id != node_id1 &&
 	    peer_md[node_id1].bitmap_uuid != peer_bm_uuid) {
! manual
-		copy_bitmap(device, from_id, node_id1);
+		copy_bitmap(device, from_id, node_id1, spin_lock_irq_flags_p);
 		modified = true;
 
 	}
 	if (from_id != node_id2 &&
 	    peer_md[node_id2].bitmap_uuid != peer_bm_uuid) {
! manual
-		copy_bitmap(device, from_id, node_id2);
+		copy_bitmap(device, from_id, node_id2, spin_lock_irq_flags_p);
 		modified = true;
 	}
 
 	return modified;
 }
 
! header
-void drbd_uuid_detect_finished_resyncs(struct drbd_peer_device *peer_device) __must_hold(local)
+void drbd_uuid_detect_finished_resyncs(struct drbd_peer_device *peer_device) 
 {
! cocci
+	KIRQL spin_lock_irq_flags;
 	u64 peer_current_uuid = peer_device->current_uuid & ~UUID_PRIMARY;
 	struct drbd_device *device = peer_device->device;
 	struct drbd_peer_md *peer_md = device->ldev->md.peers;
@@ -4968,7 +5077,7 @@
 	current_equal = peer_current_uuid == (drbd_current_uuid(device) & ~UUID_PRIMARY) &&
 		!(peer_device->uuid_flags & UUID_FLAG_SYNC_TARGET);
 
! cocci
-	spin_lock_irq(&device->ldev->md.uuid_lock);
+	spin_lock_irqsave(&device->ldev->md.uuid_lock, spin_lock_irq_flags);
 
 	if (peer_device->repl_state[NOW] == L_OFF && current_equal) {
 		u64 bm_to_peer = peer_device->comm_bitmap_uuid & ~UUID_PRIMARY;
@@ -4984,7 +5093,8 @@
 			drbd_info(peer_device, "Missed end of resync as sync-source\n");
 			set_bit(RS_SOURCE_MISSED_END, &peer_device->flags);
 		}
! manual
-		spin_unlock_irq(&device->ldev->md.uuid_lock);
+		spin_unlock_irqrestore(&device->ldev->md.uuid_lock,
+				       spin_lock_irq_flags);
 		return;
 	}
 
@@ -5011,7 +5121,7 @@
 				if (node_id == peer_device->node_id)
 					drbd_print_uuids(peer_device, "updated UUIDs");
 				else if (peer_md[node_id].flags & MDF_HAVE_BITMAP)
! manual
-					forget_bitmap(device, node_id);
+					forget_bitmap(device, node_id, &spin_lock_irq_flags);
 				else
 					drbd_info(device, "Clearing bitmap UUID for node %d\n",
 						  node_id);
@@ -5025,7 +5135,7 @@
 					 peer_md[node_id].bitmap_dagtag)) {
 				if (peer_md[node_id].flags & MDF_HAVE_BITMAP &&
 				    peer_md[from_node_id].flags & MDF_HAVE_BITMAP)
! manual
-					copy_bitmap(device, from_node_id, node_id);
+					copy_bitmap(device, from_node_id, node_id, &spin_lock_irq_flags);
 				else
 					drbd_info(device, "Node %d synced up to node %d.\n",
 						  node_id, from_node_id);
@@ -5035,8 +5145,9 @@
 		}
 	}
 
-	write_bm |= detect_copy_ops_on_peer(peer_device);
-	spin_unlock_irq(&device->ldev->md.uuid_lock);
! manual
+	write_bm |= detect_copy_ops_on_peer(peer_device, &spin_lock_irq_flags);
! cocci
+	spin_unlock_irqrestore(&device->ldev->md.uuid_lock,
+			       spin_lock_irq_flags);
 
 	if (write_bm || filled) {
 		u64 to_nodes = filled ? -1 : ~NODE_MASK(peer_device->node_id);
@@ -5050,7 +5161,7 @@
 }

! header
 int drbd_bmio_set_all_n_write(struct drbd_device *device,
-			      struct drbd_peer_device *peer_device) __must_hold(local)
+			      struct drbd_peer_device *peer_device) 
 {
 	drbd_bm_set_all(device);
 	return drbd_bm_write(device, NULL);
@@ -5063,13 +5174,13 @@
  * Sets all bits in the bitmap towards one peer and writes the whole bitmap to stable storage.
  */
 int drbd_bmio_set_n_write(struct drbd_device *device,
! header
-			  struct drbd_peer_device *peer_device) __must_hold(local)
+			  struct drbd_peer_device *peer_device) 
 {
 	int rv = -EIO;
 
 	drbd_md_set_peer_flag(peer_device, MDF_PEER_FULL_SYNC);
 	drbd_md_sync(device);
! cocci
-	drbd_bm_set_many_bits(peer_device, 0, -1UL);
+	drbd_bm_set_many_bits(peer_device, 0, ((ULONG_PTR)-1));
 
 	rv = drbd_bm_write(device, NULL);
 
@@ -5088,7 +5199,7 @@
  * Sets all bits in all allocated bitmap slots and writes it to stable storage.
  */
 int drbd_bmio_set_allocated_n_write(struct drbd_device *device,
! header
-				    struct drbd_peer_device *peer_device) __must_hold(local)
+				    struct drbd_peer_device *peer_device) 
 {
 	const int my_node_id = device->resource->res_opts.node_id;
 	struct drbd_md *md = &device->ldev->md;
@@ -5101,7 +5212,7 @@
 		bitmap_index = md->peers[node_id].bitmap_index;
 		if (bitmap_index == -1)
 			continue;
! cocci
-		_drbd_bm_set_many_bits(device, bitmap_index, 0, -1UL);
+		_drbd_bm_set_many_bits(device, bitmap_index, 0, ((ULONG_PTR)-1));
 	}
 	rv = drbd_bm_write(device, NULL);
 
@@ -5115,7 +5226,7 @@
  * Clears all bits in the bitmap and writes the whole bitmap to stable storage.
  */
 int drbd_bmio_clear_all_n_write(struct drbd_device *device,
! header
-			    struct drbd_peer_device *peer_device) __must_hold(local)
+			    struct drbd_peer_device *peer_device) 
 {
 	drbd_resume_al(device);
 	drbd_bm_clear_all(device);
@@ -5154,12 +5265,13 @@
 
 void drbd_queue_pending_bitmap_work(struct drbd_device *device)
 {
! cocci (must not use 'flags' as generated identifier)
-	unsigned long flags;
+	KIRQL flags;
+	KIRQL flags2;
 
! cocci (must not use 'flags' as generated identifier)
 	spin_lock_irqsave(&device->pending_bitmap_work.q_lock, flags);
-	spin_lock(&device->resource->work.q_lock);
+	spin_lock_irqsave(&device->resource->work.q_lock, flags2);
 	list_splice_tail_init(&device->pending_bitmap_work.q, &device->resource->work.q);
! cocci (must not use 'flags' as generated identifier)
-	spin_unlock(&device->resource->work.q_lock);
+	spin_unlock_irqrestore(&device->resource->work.q_lock, flags2);
 	spin_unlock_irqrestore(&device->pending_bitmap_work.q_lock, flags);
 	wake_up(&device->resource->work.q_wait);
 }
@@ -5185,11 +5297,12 @@
 			  char *why, enum bm_flag flags,
 			  struct drbd_peer_device *peer_device)
 {
! cocci
+	KIRQL spin_lock_irq_flags;
 	struct bm_io_work *bm_io_work;
 
 	D_ASSERT(device, current == device->resource->worker.task);
 
! remove
-	bm_io_work = kmalloc(sizeof(*bm_io_work), GFP_NOIO);
+	bm_io_work = kmalloc(sizeof(*bm_io_work), GFP_NOIO, '0BWD');
 	if (!bm_io_work) {
 		if (done)
 			done(device, peer_device, -ENOMEM);
@@ -5234,9 +5347,11 @@
 	 * when it is only half-queued yet */
 	atomic_inc(&device->ap_bio_cnt[WRITE]);
 	atomic_inc(&device->pending_bitmap_work.n);
! cocci
-	spin_lock_irq(&device->pending_bitmap_work.q_lock);
+	spin_lock_irqsave(&device->pending_bitmap_work.q_lock,
+			  spin_lock_irq_flags);
 	list_add_tail(&bm_io_work->w.list, &device->pending_bitmap_work.q);
! cocci
-	spin_unlock_irq(&device->pending_bitmap_work.q_lock);
+	spin_unlock_irqrestore(&device->pending_bitmap_work.q_lock,
+			       spin_lock_irq_flags);
 	dec_ap_bio(device, WRITE);  /* may move to actual work queue */
 }
 
@@ -5281,7 +5396,7 @@
 	return rv;
 }
 
! header
-void drbd_md_set_flag(struct drbd_device *device, enum mdf_flag flag) __must_hold(local)
+void drbd_md_set_flag(struct drbd_device *device, enum mdf_flag flag) 
 {
 	if ((device->ldev->md.flags & flag) != flag) {
 		drbd_md_mark_dirty(device);
@@ -5290,7 +5405,7 @@
 }
 
! cocci
 void drbd_md_set_peer_flag(struct drbd_peer_device *peer_device,
-			   enum mdf_peer_flag flag) __must_hold(local)
+			   enum mdf_peer_flag flag) 
 {
 	struct drbd_device *device = peer_device->device;
 	struct drbd_md *md = &device->ldev->md;
@@ -5301,7 +5416,7 @@
 	}
 }
 
! cocci
-void drbd_md_clear_flag(struct drbd_device *device, enum mdf_flag flag) __must_hold(local)
+void drbd_md_clear_flag(struct drbd_device *device, enum mdf_flag flag) 
 {
 	if ((device->ldev->md.flags & flag) != 0) {
 		drbd_md_mark_dirty(device);
@@ -5310,7 +5425,7 @@
 }
 
! cocci
 void drbd_md_clear_peer_flag(struct drbd_peer_device *peer_device,
-			     enum mdf_peer_flag flag) __must_hold(local)
+			     enum mdf_peer_flag flag) 
 {
 	struct drbd_device *device = peer_device->device;
 	struct drbd_md *md = &device->ldev->md;
@@ -5338,7 +5453,7 @@
 
 static void md_sync_timer_fn(struct timer_list *t)
 {
! cocci (typeof)
-	struct drbd_device *device = from_timer(device, t, md_sync_timer);
+	struct drbd_device *device = from_timer(device, t, md_sync_timer, struct drbd_device);
 	drbd_device_post_work(device, MD_SYNC);
 }
 
@@ -5350,16 +5465,17 @@
  * @i:		the struct drbd_interval embedded in struct drbd_request or
  *		struct drbd_peer_request
  */
! manual
-int drbd_wait_misc(struct drbd_device *device, struct drbd_peer_device *peer_device, struct drbd_interval *i)
+int drbd_wait_misc(struct drbd_device *device, struct drbd_peer_device *peer_device, struct drbd_interval *i, KIRQL *spin_lock_irq_flags_p)
 {
! cocci
+	KIRQL rcu_flags;
 	DEFINE_WAIT(wait);
! cocci
-	long timeout;
+	LONG_PTR timeout;
 
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	if (peer_device) {
 		struct net_conf *net_conf = rcu_dereference(peer_device->connection->transport.net_conf);
 		if (!net_conf) {
! cocci
-			rcu_read_unlock();
+			rcu_read_unlock(rcu_flags);
 			return -ETIMEDOUT;
 		}
 		timeout = net_conf->ko_count ? net_conf->timeout * HZ / 10 * net_conf->ko_count :
@@ -5368,15 +5484,15 @@
 		struct disk_conf *disk_conf = rcu_dereference(device->ldev->disk_conf);
 		timeout = disk_conf->disk_timeout * HZ / 10;
 	}
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	/* Indicate to wake up device->misc_wait on progress.  */
 	i->waiting = true;
 	prepare_to_wait(&device->misc_wait, &wait, TASK_INTERRUPTIBLE);
! cocci
-	spin_unlock_irq(&device->resource->req_lock);
+	spin_unlock_irqrestore(&device->resource->req_lock, *spin_lock_irq_flags_p);
 	timeout = schedule_timeout(timeout);
 	finish_wait(&device->misc_wait, &wait);
! cocci
-	spin_lock_irq(&device->resource->req_lock);
+	spin_lock_irqsave(&device->resource->req_lock, *spin_lock_irq_flags_p);
 	if (!timeout || (peer_device && peer_device->repl_state[NOW] < L_ESTABLISHED))
 		return -ETIMEDOUT;
 	if (signal_pending(current))
@@ -5405,23 +5521,24 @@
 	mutex_unlock(&resources_mutex);
 }
 
! cocci
-long twopc_timeout(struct drbd_resource *resource)
+LONG_PTR twopc_timeout(struct drbd_resource *resource)
 {
 	return resource->res_opts.twopc_timeout * HZ/10;
 }
 
 u64 directly_connected_nodes(struct drbd_resource *resource, enum which_state which)
 {
! cocci
+	KIRQL rcu_flags;
 	u64 directly_connected = 0;
 	struct drbd_connection *connection;
 
! cocci
-	rcu_read_lock();
+	rcu_flags = rcu_read_lock();
 	for_each_connection_rcu(connection, resource) {
 		if (connection->cstate[which] < C_CONNECTED)
 			continue;
 		directly_connected |= NODE_MASK(connection->peer_node_id);
 	}
! cocci
-	rcu_read_unlock();
+	rcu_read_unlock(rcu_flags);
 
 	return directly_connected;
 }
@@ -5505,8 +5622,8 @@
 /* Fault insertion support including random number generator shamelessly
  * stolen from kernel/rcutorture.c */
 struct fault_random_state {
! cocci
-	unsigned long state;
-	unsigned long count;
+	ULONG_PTR state;
+	ULONG_PTR count;
 };
 
 #define FAULT_RANDOM_MULT 39916801  /* prime */
@@ -5520,7 +5637,7 @@
 static unsigned long
 _drbd_fault_random(struct fault_random_state *rsp)
 {
! cocci
-	long refresh;
+	LONG_PTR refresh;
 
 	if (!rsp->count--) {
 		get_random_bytes(&refresh, sizeof(refresh));
