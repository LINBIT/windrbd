From 55da8941e8b504f0b5cba0f397333b17bd44cde9 Mon Sep 17 00:00:00 2001
From: Johannes Thoma <johannes@johannesthoma.com>
Date: Fri, 4 Dec 2020 17:15:05 +0100
Subject: [PATCH 2/4] drbd_receiver: patch set for speeding up sync then
 SyncTarget.

We use 'big pages': since pages are regular memory allocated by
kmalloc() we can have pages of any size. With this patch we do not
split resync requests > 4K into 4K blocks (which would be submitted
one per one to the backing device, which makes it slow). Instead
we handle the resync request with one big pages (and also submit
that one big page to the backing device).
---
 drbd/drbd_receiver.c | 42 +++++++++++++++++++++++++++++-------------
 1 file changed, 29 insertions(+), 13 deletions(-)

diff --git a/drbd/drbd_receiver.c b/drbd/drbd_receiver.c
index a1731ce..f741240 100644
--- a/drbd/drbd_receiver.c
+++ b/drbd/drbd_receiver.c
@@ -287,27 +287,39 @@ static void page_chain_add(struct page **head,
 	*head = chain_first;
 }
 
-static struct page *__drbd_alloc_pages(unsigned int number, gfp_t gfp_mask)
+static struct page *__drbd_alloc_pages(unsigned int number, gfp_t gfp_mask, int use_big_pages)
 {
 	KIRQL spin_lock_flags;
 	struct page *page = NULL;
 	struct page *tmp = NULL;
 	unsigned int i = 0;
+	size_t len;
 
 	/* Yes, testing drbd_pp_vacant outside the lock is racy.
 	 * So what. It saves a spin_lock. */
-	if (drbd_pp_vacant >= number) {
+	while (drbd_pp_vacant >= number && !use_big_pages) {
 		spin_lock_irqsave(&drbd_pp_lock, spin_lock_flags);
 		page = page_chain_del(&drbd_pp_pool, number);
-		if (page)
+		if (page) {
 			drbd_pp_vacant -= number;
+			if (page->size > PAGE_SIZE) {
+				spin_unlock_irqrestore(&drbd_pp_lock, spin_lock_flags);
+				continue;
+			}
+		}
 		spin_unlock_irqrestore(&drbd_pp_lock, spin_lock_flags);
 		if (page)
 			return page;
 	}
+	if (use_big_pages) {
+		len = PAGE_SIZE * number;
+		number = 1;
+	} else {
+		len = PAGE_SIZE;
+	}
 
 	for (i = 0; i < number; i++) {
-		tmp = alloc_page(gfp_mask);
+		tmp = alloc_page_of_size(gfp_mask, len);
 		if (!tmp)
 			break;
 		set_page_chain_next_offset_size(tmp, page, 0, 0);
@@ -394,7 +406,7 @@ static void drbd_reclaim_net_peer_reqs(struct drbd_connection *connection)
  * Returns a page chain linked via (struct drbd_page_chain*)&page->lru.
  */
 struct page *drbd_alloc_pages(struct drbd_transport *transport, unsigned int number,
-			      gfp_t gfp_mask)
+			      gfp_t gfp_mask, int use_big_pages)
 {
 	KIRQL rcu_flags;
 	struct drbd_connection *connection =
@@ -408,7 +420,7 @@ struct page *drbd_alloc_pages(struct drbd_transport *transport, unsigned int num
 	rcu_read_unlock(rcu_flags);
 
 	if (atomic_read(&connection->pp_in_use) < mxb)
-		page = __drbd_alloc_pages(number, gfp_mask & ~__GFP_RECLAIM);
+		page = __drbd_alloc_pages(number, gfp_mask & ~__GFP_RECLAIM, use_big_pages);
 
 	/* Try to keep the fast path fast, but occasionally we need
 	 * to reclaim the pages we lent to the network stack. */
@@ -421,7 +433,7 @@ struct page *drbd_alloc_pages(struct drbd_transport *transport, unsigned int num
 		drbd_reclaim_net_peer_reqs(connection);
 
 		if (atomic_read(&connection->pp_in_use) < mxb) {
-			page = __drbd_alloc_pages(number, gfp_mask);
+			page = __drbd_alloc_pages(number, gfp_mask, use_big_pages);
 			if (page)
 				break;
 		}
@@ -439,8 +451,12 @@ struct page *drbd_alloc_pages(struct drbd_transport *transport, unsigned int num
 	}
 	finish_wait(&drbd_pp_wait, &wait);
 
-	if (page)
-		atomic_add(number, &connection->pp_in_use);
+	if (page) {
+		if (use_big_pages)
+			atomic_inc(&connection->pp_in_use);
+		else
+			atomic_add(number, &connection->pp_in_use);
+	}
 	return page;
 }
 
@@ -1734,17 +1750,17 @@ next_bio:
 	++n_bios;
 
 	page_chain_for_each(page) {
-		unsigned off, len;
+		unsigned int off, len;
 		int res;
 
 		if (peer_req_op(peer_req) == REQ_OP_READ) {
 			set_page_chain_offset(page, 0);
-			set_page_chain_size(page, min_t(unsigned, data_size, PAGE_SIZE));
+			set_page_chain_size(page, min_t(unsigned int, data_size, page->size));
 		}
 		off = page_chain_offset(page);
 		len = page_chain_size(page);
 
-		if (off > PAGE_SIZE || len > PAGE_SIZE - off || len > data_size || len == 0) {
+		if (off > page->size || len > page->size - off || len > data_size || len == 0) {
 			drbd_err(device, "invalid page chain: offset %u size %u remaining data_size %u\n",
 					off, len, data_size);
 			err = -EINVAL;
@@ -3361,7 +3377,7 @@ static int receive_DataRequest(struct drbd_connection *connection, struct packet
 		goto fail;
 	if (size) {
 		drbd_alloc_page_chain(&peer_device->connection->transport,
-			&peer_req->page_chain, DIV_ROUND_UP(size, PAGE_SIZE), GFP_TRY);
+			&peer_req->page_chain, DIV_ROUND_UP(size, PAGE_SIZE), GFP_TRY, 0);
 		if (!peer_req->page_chain.head)
 			goto fail2;
 	}
-- 
2.17.0

